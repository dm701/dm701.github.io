<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-05-17 Fri 21:00 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>dm701</title>
<meta name="author" content="dm" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/js/readtheorg.js"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">dm701</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgb9cc023">1. Github repositories</a></li>
<li><a href="#org17eb1b2">2. Machine learning and deep learning</a>
<ul>
<li><a href="#org708d982">2.1. Little learner</a>
<ul>
<li><a href="#org66c3199">2.1.1. Functions short definitions</a></li>
<li><a href="#orgde32033">2.1.2. Gradients</a></li>
<li><a href="#org6e82853">2.1.3. Rate of change</a></li>
<li><a href="#org2d775a1">2.1.4. RMSProp</a></li>
<li><a href="#org860c051">2.1.5. Tensors</a></li>
<li><a href="#org72e1185">2.1.6. Update of parameters</a></li>
<li><a href="#org36681e1">2.1.7. Velocity</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgb9cc023" class="outline-2">
<h2 id="orgb9cc023"><span class="section-number-2">1.</span> Github repositories</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li><a href="https://github.com/dm701/AI-for-media">AI for media</a></li>
<li><a href="https://github.com/dm701/Data-Science-assignments">Data science assignments</a></li>
<li><a href="https://github.com/dm701/NLP-project">NLP project</a></li>
<li><a href="https://github.com/dm701/Personalisation-and-machine-learning">Personalisation and machine learning</a></li>
<li><a href="https://github.com/dm701/STEM-assignments">STEM assignments</a></li>
</ul>
</div>
</div>
<div id="outline-container-org17eb1b2" class="outline-2">
<h2 id="org17eb1b2"><span class="section-number-2">2.</span> Machine learning and deep learning</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org708d982" class="outline-3">
<h3 id="org708d982"><span class="section-number-3">2.1.</span> Little learner</h3>
<div class="outline-text-3" id="text-2-1">
<p>
This section is a series of notes about machine learning and deep
learning in the context of the book
<a href="https://www.thelittlelearner.com/">"The Little Learner: A straight line to deep learning"</a> by
<i>Daniel P. Friedman and Anurag Mendhekar</i>
</p>
</div>
<div id="outline-container-org66c3199" class="outline-4">
<h4 id="org66c3199"><span class="section-number-4">2.1.1.</span> Functions short definitions</h4>
<div class="outline-text-4" id="text-2-1-1">
<ul class="org-ul">
<li><p>
<span class="underline">Expectant and objective functions</span>
</p>

<p>
A function invocation such as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(l2-loss line)
</pre>
</div>

<p>
which in a "same-as" chart transcribes as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(lambda (xs ys)
  (lambda (theta)
    (let ((pred-ys ((line xs) theta)))
      (sum
       (sqr
        (- ys pred-ys)))))
</pre>
</div>

<p>
produces another function. This function which is produced when
<b>l2-loss</b> is invoked with a <b>target</b> function (<b>line</b> in this case),
is referred to as an <b>expectant</b> function. This is because it is
<b>expecting</b> a data set as arguments.
</p>

<p>
When an expectant function receives a data set, for example:
</p>

<div class="org-src-container">
<pre class="src src-racket">((l2-loss line) line-xs line-ys)
</pre>
</div>

<p>
which in a "same-as" chart transcribes as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(lambda (theta)
  (let ((pred-ys ((line line-xs) theta)))
    (sum
     (sqr
      (- line-ys pred-ys)))))
</pre>
</div>

<p>
produces a function which <b>awaits</b> a theta. The name of the produced
function is known as an <b>objective</b> function. When provided with a
theta, the objective function returns a scalar representing the
<b>loss</b>, which is a measure of how far away we are from the well fitted
theta.
</p>

<p>
The objective function would be called as such:
</p>

<div class="org-src-container">
<pre class="src src-racket">(((l2-loss line) line-xs line-ys) (list 0.0 0.0))
</pre>
</div></li>

<li><p>
<span class="underline">Parameterized functions</span>
</p>

<p>
Functions such as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(define line
  (lambda (x)
    (lambda (theta)
      (+ (* (ref theta 0) x) (ref theta 1)))))
</pre>
</div>

<p>
are known as <b>parameterized functions</b>.
</p>

<p>
Parameterized functions are used where we must figure out the right
values for the parameters (here, theta 0 and theta 1) from given
values of x and the corresponding values of y.
</p>

<p>
In this case when line is invoked, for example, with the <b>argument</b> 8
as such:
</p>

<div class="org-src-container">
<pre class="src src-racket">(line 8)
</pre>
</div>

<p>
we can say that it is waiting to accept arguments for its parameters
<b>theta 0</b> and <b>theta 1</b>.
</p>

<p>
Another thing to note is that when (line 8) is invoked on theta 0 and
theta 1 as such:
</p>

<div class="org-src-container">
<pre class="src src-racket">((line 8) 4 6)
</pre>
</div>

<p>
we can then determine y.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-orgde32033" class="outline-4">
<h4 id="orgde32033"><span class="section-number-4">2.1.2.</span> Gradients</h4>
<div class="outline-text-4" id="text-2-1-2">
<ul class="org-ul">
<li><p>
<span class="underline">Gradient short definition</span>
</p>

<p>
A gradient is a general way of understanding the rate of change of a
parameterized function with respect to <b>all</b> its parameters.
</p></li>

<li><p>
<span class="underline">Gradient fancy name</span>
</p>

<p>
The gradient is a fancy word for derivative, or the rate of change of
a function.
</p>

<p>
The term "gradient" is typically used for functions with several
inputs and a single output (a scalar field). Yes, you can say a line
has a gradient (its slope), but using "gradient" for single-variable
functions is unnecessarily confusing.
</p></li>

<li><p>
<span class="underline">Gradient-of function short explanation</span>
</p>

<p>
The result of the "gradient-of" function is a list of gradients of the
<b>objective function</b> f with respect to each parameter in theta, and is
referred to as the gradient list.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org6e82853" class="outline-4">
<h4 id="org6e82853"><span class="section-number-4">2.1.3.</span> Rate of change</h4>
<div class="outline-text-4" id="text-2-1-3">
<ul class="org-ul">
<li><p>
<span class="underline">Rate of change short definition</span>
</p>

<p>
The rate of change of a function (of the <b>objective</b> function in most
cases), such as:
</p>

<div class="org-src-container">
<pre class="src src-racket">((l2-loss line) line-xs line-ys)
</pre>
</div>

<p>
determines how its result changes when its argument (i.e. theta) is
revised.
</p>

<p>
The rate of change is also known as the <b>derivative</b>.
</p>

<p>
A more concrete example of this would be to invoke an objective
function as such:
</p>

<div class="org-src-container">
<pre class="src src-racket">(((l2-loss line) line-xs line-ys) (list 0.0 0.0))
</pre>
</div>

<p>
which would achieve the objective of finding a well-fitted theta by
returning the <b>loss</b> for this particular theta.
</p>

<p>
If the returned loss was, for instance, 33.21, we would test the
behaviour of "theta 0" to see how we should revise it. We then change
"theta 0" by increasing it a small amount for testing purposes, so
that our new "theta 0 is" 0.0099. If the loss goes down, for instance,
to 32.59, we are slightly closer to our ideal loss. In other words we
changed the loss by:
</p>

<p>
(32.59 − 33.21) = −0.62
</p>

<p>
Now that increasing our "theta 0" by 0.0099 has changed our loss by
-0.62 we would say that our <b>rate of change</b> is:
</p>

<p>
-0.62 / 0.0099 = -62.63
</p></li>

<li><p>
<span class="underline">Rate of change calculation</span>
</p>

<p>
The rate of change is determined by subtracting the old (which so far
has been greater) loss from the new (which so far has been smaller)
loss, and in our examples so far this has resulted in negative values.
</p></li>

<li><p>
<span class="underline">Using the rate of change</span>
</p>

<p>
Increasing theta from 0.0 by a small value can result in a rate of
change which has a large <b>absolute value</b>, meaning that a small
increase in theta causes a relatively large decrease in its loss.
</p>

<p>
This idea can be used to determine how much further to revise theta so
as to achieve a bigger loss. However we should be wary that the
revision of theta moves us <b>closer</b> but does not <b>overshoot</b> the ideal
loss.
</p>

<p>
This problem can be resolved by taking a small scalar (like 0.01),
<b>and multiply the rate of change by it and revise our theta by that amount</b>.
</p>

<p>
This small scalar is known as the <b>learning rate</b>.
</p></li>

<li><p>
<span class="underline">Theta revision after finding the rate of change</span>
</p>

<p>
The rate of change is multiplied by alpha (the learning rate) and the
returned (negative) value used to update/revise theta by subtracting
from theta this negative value (which has resulted in a positive
revision of theta so far).
</p>

<p>
The rate of change <b>cannot</b> be reused as it depends on the current
theta.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org2d775a1" class="outline-4">
<h4 id="org2d775a1"><span class="section-number-4">2.1.4.</span> RMSProp</h4>
<div class="outline-text-4" id="text-2-1-4">
<ul class="org-ul">
<li><p>
<span class="underline">RMSProp short definition</span>
</p>

<p>
This algorithm works by modifying the <b>fraction</b> of the <b>gradient</b> used at
each revision.
</p>

<p>
Since our alpha so far has been a constant, we know that it causes the
velocity of the gradient descent to slow down in a similar way.
</p>

<p>
Because alpha represents the fraction of the gradient we're going to
use as our velocity, another approach to addressing this problem is to
make this fraction <b>adaptive</b>. Adaptive here means that the fraction
is decided based on the gradient and its <b>historical</b> values.
</p></li>

<li><p>
<span class="underline">RMSProp reason for squaring gradient in smooth invocation</span>
The gradient g can be negative, and if we get too many consecutive
negative gradients, then our historical averages can themselves become
negative.
</p>

<p>
This is a problem because r gets used by the <b>modifier</b>" G i.e.
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ (sqrt r) epsilon)
</pre>
</div>

<p>
and its being negative can make alpha-hat (i.e. the learning rate)
negative. When that happens, we end up ascending the gradient instead
of descending it.
</p>

<p>
This means that we would move our theta in a direction that
<b>increases</b> the loss instead of a direction that <b>decreases</b> the loss.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org860c051" class="outline-4">
<h4 id="org860c051"><span class="section-number-4">2.1.5.</span> Tensors</h4>
<div class="outline-text-4" id="text-2-1-5">
<ul class="org-ul">
<li><p>
Here is a tensor<sup>1</sup>. A tensor<sup>1</sup> has only <b>scalars</b> and groups scalars
together:
</p>

<div class="org-src-container">
<pre class="src src-racket">[5.0 7.18 3.1416]
</pre>
</div></li>

<li><p>
A tensor<sup>2</sup> can be thought of as a <b>matrix</b> or a <b>two-dimensional
array</b>. The elements of a tensor<sup>2</sup> are tensors<sup>1</sup>, for example:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[7 6 2 5] [3 8 6 9] [9 4 6 5]]
</pre>
</div>

<p>
has 3 elements:
</p>

<div class="org-src-container">
<pre class="src src-racket">[7 6 2 5]
[3 8 6 9]
</pre>
</div>

<p>
and:
</p>

<div class="org-src-container">
<pre class="src src-racket">[9 4 6 5]
</pre>
</div>

<p>
Therefore if we have a tensor whose elements are tensors<sup>m</sup>, that
makes it a tensor<sup>m+1</sup>. One condition however is that all the
tensors<sup>m</sup> must have the same numbers of elements.
</p></li>

<li><p>
A scalar such as:
</p>

<div class="org-src-container">
<pre class="src src-racket">9
</pre>
</div>

<p>
is also a tensor. It is a tensor<sup>0</sup>, but <b>zero-dimensional</b> arrays
are rarely mentioned.
</p></li>

<li><p>
The above superscripts have a name, they are known as the <b>rank</b> of
the tensor. The rank of a tensor tells us how deeply nested its
elements are. For instance, here is a tensor<sup>3</sup>:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[[8 9] [4 7]]]
</pre>
</div>

<p>
this is because it has 1 tensor<sup>2</sup> element that has 2 tensor<sup>1</sup>
elements of 2 scalars each.
</p></li>

<li><p>
In any given tensor, the nested tensors have the same number of
elements. For example, the nested tensors of tensors<sup>2</sup> are all
tensors<sup>1</sup>, and each of those tensors<sup>1</sup> has the same number of
tensors<sup>0</sup>.
</p>

<p>
This means that the tensors<sup>m</sup> that are elements of a tensor<sup>m+1</sup>
have the same <b>shape</b>.
</p></li>

<li><p>
The <b>shape</b> of:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[5.2 6.3 8.0] [6.9 7.1 0.5]]
</pre>
</div>

<p>
is this list of positive natural numbers:
</p>

<div class="org-src-container">
<pre class="src src-racket">(list 2 3)
</pre>
</div>

<p>
because it is a tensor<sup>2</sup> of 2 tensors<sup>1</sup>, each of which has 3
tensors<sup>0</sup> elements.
</p>

<p>
The shape of:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[[5] [6] [8]] [[7] [9] [5]]]
</pre>
</div>

<p>
is:
</p>

<div class="org-src-container">
<pre class="src src-racket">(list 2 3 1)
</pre>
</div>

<p>
It is:
</p>

<ul class="org-ul">
<li>a tensor<sup>3</sup> of 2 tensor<sup>2</sup> elements.</li>

<li>Each of those tensor<sup>2</sup> has 3 tensor<sup>1</sup> elements.</li>

<li>Each of those tensor<sup>1</sup> has 1 tensor<sup>0</sup> element, which is a scalar.</li>
</ul></li>

<li>Another useful thing to note is that the rank of a tensor is equal
to the length of its shape.</li>
</ul>
</div>
</div>
<div id="outline-container-org72e1185" class="outline-4">
<h4 id="org72e1185"><span class="section-number-4">2.1.6.</span> Update of parameters</h4>
<div class="outline-text-4" id="text-2-1-6">
<p>
The following function:
</p>

<div class="org-src-container">
<pre class="src src-racket">(define naked-u
  (lambda (P g)
    (− P (* alpha g))))
</pre>
</div>

<p>
updates the parameters by multiplying the gradient g by the learning
rate alpha, and subtracts the result from the parameter P to yield the
next P, so that ultimately we get closer to a well-fitted theta.
</p>
</div>
</div>
<div id="outline-container-org36681e1" class="outline-4">
<h4 id="org36681e1"><span class="section-number-4">2.1.7.</span> Velocity</h4>
<div class="outline-text-4" id="text-2-1-7">
<ul class="org-ul">
<li><p>
<span class="underline">Velocity of descent short definition</span>
</p>

<p>
The <b>change</b> that we make to a given parameter at each revision is known
as the <b>velocity of descent</b>.
</p>

<p>
In the following expression:
</p>

<div class="org-src-container">
<pre class="src src-racket">(define naked-u
  (lambda (P g)
    (− P (* alpha g))))
</pre>
</div>

<p>
since we subtract (* alpha g), the change to P, (i.e., the velocity)
is:
</p>

<div class="org-src-container">
<pre class="src src-racket">(− (* alpha g))
</pre>
</div></li>

<li><p>
<span class="underline">Velocity of descent slowing concept</span>
</p>

<p>
We can observe from a loss graph with tangents, that as each tangent
approaches the lowest point on a graph, they get less and less steep
towards the bottom of the curve i.e. their slope (<b>the gradient</b>) gets
smaller.
</p>

<p>
In fact, as the curve's bottom is approached, the gradient gets closer
and closer to 0.0.
</p>

<p>
What happens when we multiply a really small gradient with a really
small learning rate as we do:
</p>

<div class="org-src-container">
<pre class="src src-racket">(* alpha g)
</pre>
</div>

<p>
in update functions is that we get something even smaller. So at each
revision closer to the bottom, the amount of change to each parameter
gets smaller and smaller.
</p>

<p>
Therefore we can say the <b>velocity of descent</b> slows down as we
approach the bottom of the curve.
</p></li>

<li><p>
<span class="underline">Velocity of descent speed up concept</span>
</p>

<p>
The problem of the <b>velocity of descent</b> slowing down can be remedied
by boosting our velocity. This is achieved by adding some fraction
<b>mu</b> of the velocity <b>v</b>, of the previous revision, to the change we
expect to make in the current revision.
</p>

<p>
Therefore our velocity which was:
</p>

<div class="org-src-container">
<pre class="src src-racket">(− (* alpha g))
</pre>
</div>

<p>
becomes:
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ (* mu v) (- (* alpha g)))
</pre>
</div>

<p>
which is better written as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(- (* mu v) (* alpha g))
</pre>
</div>

<p>
Here, <b>v</b> represents the velocity of the most recent revision.
</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: dm</p>
<p class="date">Created: 2024-05-17 Fri 21:00</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
