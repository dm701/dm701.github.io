<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-06-01 Sat 06:56 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>dm701</title>
<meta name="author" content="dm" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/js/readtheorg.js"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">dm701</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgb9cc023">Github repositories</a></li>
<li><a href="#org17eb1b2">Machine learning and deep learning</a>
<ul>
<li><a href="#org8b0eb00">General</a>
<ul>
<li><a href="#org5652949">Elementwise operations</a></li>
<li><a href="#org4cf7bc6">Hadamard product</a></li>
<li><a href="#org99c01cb">Outliers</a></li>
<li><a href="#org87f96a4">Tensor broadcasting</a></li>
</ul>
</li>
<li><a href="#org708d982">Little learner</a>
<ul>
<li><a href="#orgb119b63">Functions</a>
<ul>
<li><a href="#org4444a28">Expectant and objective functions</a></li>
<li><a href="#org38cd5df">Extended functions</a></li>
<li><a href="#orgee35afe">Loss function</a></li>
<li><a href="#org8b98f70">Parameterized functions</a></li>
</ul>
</li>
<li><a href="#orgde32033">Gradients</a>
<ul>
<li><a href="#org9639605">Nomenclature</a></li>
<li><a href="#org7161a33">Gradient descent</a></li>
</ul>
</li>
<li><a href="#org6e82853">Rate of change</a></li>
<li><a href="#org2d775a1">RMSProp</a></li>
<li><a href="#org860c051">Tensors</a>
<ul>
<li><a href="#orge231765">Tensor rank</a></li>
<li><a href="#orgd4ebb43">Tensor shape</a></li>
</ul>
</li>
<li><a href="#org72e1185">Update of parameters</a></li>
<li><a href="#org36681e1">Velocity</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgb9cc023" class="outline-2">
<h2 id="orgb9cc023">Github repositories</h2>
<div class="outline-text-2" id="text-orgb9cc023">
<ul class="org-ul">
<li><a href="https://github.com/dm701/AI-for-media">AI for media</a></li>
<li><a href="https://github.com/dm701/Data-Science-assignments">Data science assignments</a></li>
<li><a href="https://github.com/dm701/NLP-project">NLP project</a></li>
<li><a href="https://github.com/dm701/Personalisation-and-machine-learning">Personalisation and machine learning</a></li>
<li><a href="https://github.com/dm701/STEM-assignments">STEM assignments</a></li>
</ul>
</div>
</div>
<div id="outline-container-org17eb1b2" class="outline-2">
<h2 id="org17eb1b2">Machine learning and deep learning</h2>
<div class="outline-text-2" id="text-org17eb1b2">
</div>
<div id="outline-container-org8b0eb00" class="outline-3">
<h3 id="org8b0eb00">General</h3>
<div class="outline-text-3" id="text-org8b0eb00">
</div>
<div id="outline-container-org5652949" class="outline-4">
<h4 id="org5652949">Elementwise operations</h4>
<div class="outline-text-4" id="text-org5652949">
<p>
In elementwise operations like addition, subtraction, and division,
values that correspond positionally are combined to produce a new
tensor. The first value in tensor <i>A</i> is paired with the first value
in tensor <i>B</i>. The second value is paired with the second, and so
on. This means the tensors must have equal dimensions (i.e. equal
<a href="#orgd4ebb43">shapes</a>) in order to complete the operation (source:
<a href="https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#elementwise-operations">ml-cheatsheets</a>).
</p>


<div id="org7f16d57" class="figure">
<p><img src="img/Elementwise_operations.png" alt="Elementwise_operations.png" />
</p>
<p><span class="figure-number">Figure 1: </span>Elementwise operation</p>
</div>
</div>
</div>
<div id="outline-container-org4cf7bc6" class="outline-4">
<h4 id="org4cf7bc6">Hadamard product</h4>
<div class="outline-text-4" id="text-org4cf7bc6">
<p>
The <b>Hadamard product</b> (also known as <b>element wise product</b>) is an
operation which takes in two matrices of the same shape and returns a
matrix of the multiplied corresponding elements (source: <a href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices)">wikipedia</a>).
</p>
</div>
</div>
<div id="outline-container-org99c01cb" class="outline-4">
<h4 id="org99c01cb">Outliers</h4>
<div class="outline-text-4" id="text-org99c01cb">
<p>
An outlier is a data point significantly different from other data
points in a dataset. Outliers can occur for various reasons, such as
measurement errors, data entry errors, or natural variations in the
data. They can significantly impact analysis in machine learning.
</p>

<p>
An example of this is if we were to take the following dataset:
</p>

<div class="org-src-container">
<pre class="src src-racket">[15 101 18 7 13 16 11 21 5 15 10 9]
</pre>
</div>

<p>
just by looking at it, one can quickly say ‘101’ is an outlier because
it is much larger than the other values (source: <a href="https://www.almabetter.com/bytes/articles/outlier-detection-methods-and-techniques-in-machine-learning-with-examples">almabetter</a> and <a href="https://www.analyticsvidhya.com/blog/2021/05/detecting-and-treating-outliers-treating-the-odd-one-out/">analyticsvidhya</a>).
</p>
</div>
</div>
<div id="outline-container-org87f96a4" class="outline-4">
<h4 id="org87f96a4">Tensor broadcasting</h4>
<div class="outline-text-4" id="text-org87f96a4">
<p>
According to <a href="https://numpy.org/doc/stable/user/basics.broadcasting.html">numpy</a>, tensor operations are usually done on pairs of
tensors on an element-by-element basis. In the simplest case, the two
tensors must have the exact same shape. For example:
</p>

<div class="org-src-container">
<pre class="src src-racket">(* [1.0 2.0 3.0] [2.0 2.0 2.0])
</pre>
</div>

<p>
will return:
</p>

<div class="org-src-container">
<pre class="src src-racket">[2.0 4.0 6.0]
</pre>
</div>

<p>
Broadcasting rules can <b>relax</b> these constraints when the tensors'
shapes meet certain constraints. The simplest broadcasting example
occurs when a tensor and a scalar are combined in an operation:
</p>

<div class="org-src-container">
<pre class="src src-racket">(* [1.0 2.0 3.0] 2.0)
</pre>
</div>

<p>
returns:
</p>

<div class="org-src-container">
<pre class="src src-racket">[2.0 4.0 6.0]
</pre>
</div>

<p>
We can think of the scalar being <b>stretched</b> during the arithmetic
operation, into a tensor the same shape as the first one. The new
elements in the stretched tensor are just copies of the original
scalar.
</p>


<div id="orgd98eb26" class="figure">
<p><img src="img/broadcasting_1.png" alt="broadcasting_1.png" />
</p>
<p><span class="figure-number">Figure 2: </span>In the simplest example of broadcasting, the scalar <b>b</b> is stretched to become an array of same shape as <b>a</b> so the shapes are compatible for element-by-element multiplication (source: <a href="https://numpy.org/doc/stable/user/basics.broadcasting.html">numpy</a>).</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org708d982" class="outline-3">
<h3 id="org708d982">Little learner</h3>
<div class="outline-text-3" id="text-org708d982">
<p>
This section is a series of notes about machine learning and deep
learning in the context of the book <a href="https://www.thelittlelearner.com/">"The Little Learner: A straight
line to deep learning"</a> by <i>Daniel P. Friedman and Anurag Mendhekar</i>.
</p>
</div>
<div id="outline-container-orgb119b63" class="outline-4">
<h4 id="orgb119b63">Functions</h4>
<div class="outline-text-4" id="text-orgb119b63">
</div>
<div id="outline-container-org4444a28" class="outline-5">
<h5 id="org4444a28">Expectant and objective functions</h5>
<div class="outline-text-5" id="text-org4444a28">
<p>
A function invocation such as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(l2-loss line)
</pre>
</div>

<p>
which in a "same-as" chart transcribes as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(lambda (xs ys)
  (lambda (theta)
    (let ((pred-ys ((line xs) theta)))
      (sum
       (sqr
        (- ys pred-ys)))))
</pre>
</div>

<p>
produces another function. This function which is produced when
<b>l2-loss</b> is invoked with a <b>target</b> function (<b>line</b> in this case),
is referred to as an <b>expectant</b> function. This is because it is
<b>expecting</b> a data set as arguments.
</p>

<p>
When an expectant function receives a data set, for example:
</p>

<div class="org-src-container">
<pre class="src src-racket">((l2-loss line) line-xs line-ys)
</pre>
</div>

<p>
which in a "same-as" chart transcribes as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(lambda (theta)
  (let ((pred-ys ((line line-xs) theta)))
    (sum
     (sqr
      (- line-ys pred-ys)))))
</pre>
</div>

<p>
it produces a function which <b>awaits</b> a &theta;. The name of the
produced function is known as an <a id="org7925b9e"></a>*objective*
function. When provided with a &theta;, the objective function
returns a scalar representing the <a href="#orgee35afe">loss</a>, which is a measure of how
far away we are from the well fitted &theta;.
</p>

<p>
The objective function would be called as such:
</p>

<div class="org-src-container">
<pre class="src src-racket">(((l2-loss line) line-xs line-ys) (list 0.0 0.0))
</pre>
</div>

<p>
<b>Key point</b>:
</p>

<p>
One ought to be mindful that
</p>

<div class="org-src-container">
<pre class="src src-racket">((l2-loss line) line-xs line-ys)
</pre>
</div>

<p>
is <b>not</b> a direct function call. It is itself a function which takes
a &theta; as input.
</p>
</div>
</div>
<div id="outline-container-org38cd5df" class="outline-5">
<h5 id="org38cd5df">Extended functions</h5>
<div class="outline-text-5" id="text-org38cd5df">
<ul class="org-ul">
<li><p>
In the context of this book, some functions were built to work on
both scalars and tensors. They are referred as <b>extended</b>
functions.<br />
For instance, the function:
</p>

<div class="org-src-container">
<pre class="src src-racket">+
</pre>
</div>

<p>
is one of them.<br />
Other functions that work with scalars can be extended similarly.
</p></li>

<li><p>
In its "regular" (i.e. non-extended) way, the function + works as
such:
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ 1 1)
5
</pre>
</div>

<p>
However, because + is built using <b>extension</b> it can also work on
tensors:
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ [2] [7])
[9]
</pre>
</div>

<p>
here is a "same-as" chart to illustrate how this results in [9]:
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ [2] [7])
[(+ 2 7)]
[9]
</pre>
</div>

<p>
When there is a function invocation like + on tensors, we look
inside those tensors to determine the invocation's final value.
</p>

<p>
Here is another example:
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ [5 6 7] [2 0 1])
</pre>
</div>

<p>
transcribes as:
</p>

<div class="org-src-container">
<pre class="src src-racket">[(+ 5 2) (+ 6 0) (+ 7 1)]
</pre>
</div>

<p>
and results in:
</p>

<div class="org-src-container">
<pre class="src src-racket">[7 6 8]
</pre>
</div>

<p>
It's as if + <b>descends</b> into its tensor<sup>1</sup> arguments to results in
another tensor<sup>1</sup>. The last step results in the tensor<sup>1</sup> of the
values of the three sums.
</p>

<p>
We can also add 2 tensors<sup>2</sup> of the same shape, for instance:
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ [[4 6 7] [2 0 1]]
   [[1 2 2] [6 3 1]])
</pre>
</div>

<p>
transcribes as:
</p>

<div class="org-src-container">
<pre class="src src-racket">[(+ [4 6 7] [1 2 2])
 (+ [2 0 1] [6 3 1])]

[[(+ 4 1) (+ 6 2) (+ 7 2)]
 [(+ 2 6) (+ 0 3) (+ 1 1)]]
</pre>
</div>

<p>
and results in:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[5 8 9] [8 3 2]]
</pre>
</div>

<p>
The authors then go on to explain that tensors <b>must</b> be of the same
shape before they can be added together, and that getting functions
such as + to work on tensors of <b>arbitrary</b> ranks is known as
<b>pointwise extension</b>. This however seems to be more commonly known
as <a href="#org5652949">elementwise operations</a>.
</p></li>

<li><p>
The point by the authors that tensors <b>must</b> have the same shape
before computation can be performed on them is, later in the text,
contradicted and unfortunately not very well explained. Nevertheless
we are shown that the following operation can actually be performed
on tensors:
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ 4 [3 6 5])
</pre>
</div>

<p>
by doing this:
</p>

<div class="org-src-container">
<pre class="src src-racket">[(+ 4 3) (+ 4 6) (+ 4 5)]
</pre>
</div>

<p>
which results in:
</p>

<div class="org-src-container">
<pre class="src src-racket">[7 10 9]
</pre>
</div>

<p>
Another example given is:
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ [6 9 1] [[4 3 8] [7 4 7]])
</pre>
</div>

<p>
and in this case, we can look inside the tensor<sup>2</sup> argument and add
the tensor<sup>1</sup> argument just as we did in the previous addition:
</p>

<div class="org-src-container">
<pre class="src src-racket">[(+ [6 9 1] [4 3 8])
 (+ [6 9 1] [7 4 7])]

[[(+ 6 4) (+ 9 3) (+ 1 8)]
 [(+ 6 7) (+ 9 4) (+ 1 7)]]
</pre>
</div>

<p>
which results in:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[10 12 9] [13 13 8]]
</pre>
</div>

<p>
This is in fact called <a href="#org87f96a4">tensor broadcasting</a>.
</p></li>

<li><p>
The reader is subsequently introduced to the extended version of:
</p>

<div class="org-src-container">
<pre class="src src-racket">*
</pre>
</div>

<p>
with the following operation:
</p>

<div class="org-src-container">
<pre class="src src-racket">(* [[4 6 5] [6 9 7]] 3)
</pre>
</div>

<p>
and these are the steps:
</p>

<div class="org-src-container">
<pre class="src src-racket">[(* [4 6 5] 3) (* [6 9 7] 3)]

[[(* 4 3) (* 6 3) (* 5 3)]
 [(* 6 3) (* 9 3) (* 7 3)]]
</pre>
</div>

<p>
which returns:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[12 18 15] [18 27 14]]
</pre>
</div>

<p>
We are told that this is the <b>Hadamard multiplication</b>.<br />
Upon further research this does not appear to fit the definition of
the Hadamard multiplication, or rather <a href="#org4cf7bc6">Hadamard product</a>, but a
simple tensor to scalar multiplication which involves broadcasting.
</p></li>

<li><p>
Other extended functions such as:
</p>

<div class="org-src-container">
<pre class="src src-racket">sqrt
</pre>
</div>

<p>
descend into tensors. In the case of a tensor<sup>1</sup>, this is how sqrt
works:
</p>

<div class="org-src-container">
<pre class="src src-racket">(sqrt [9 16 25])

[(sqrt 9) (sqrt 16) (sqrt 25)]
</pre>
</div>

<p>
which returns:
</p>

<div class="org-src-container">
<pre class="src src-racket">[3 4 5]
</pre>
</div>

<p>
In the case of a tensor<sup>2</sup>, it works this way:
</p>

<div class="org-src-container">
<pre class="src src-racket">(sqrt [[49 81 16] [64 25 36]])

[(sqrt [[49 81 16]]) (sqrt [[64 25 36]])]

[[(sqrt 49) (sqrt 81) (sqrt 16)]
 [(sqrt 64) (sqrt 25) (sqrt 36)]]
</pre>
</div>

<p>
and this results in:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[7 9 4] [8 5 6]]
</pre>
</div>

<p>
In other words, the function descends inside each tensor<sup>1</sup> until it
finds a tensor<sup>0</sup> at which point it gets their square root.<br />
However, not all extended functions descend until they find scalars.
The function
</p>

<div class="org-src-container">
<pre class="src src-racket">sum
</pre>
</div>

<p>
which extends the function
</p>

<div class="org-src-container">
<pre class="src src-racket">sum-1
</pre>
</div>

<p>
descends into its argument until it finds a tensor<sup>1</sup> instead of a
tensor<sup>0</sup>.<br />
This is how sum-1 is defined:
</p>

<div class="org-src-container">
<pre class="src src-racket">(define sum1
  (lambda (t)
    (summed t (sub1 ) 0.0)))

(define summed
  (lambda (t i a)
    (cond
      ((zero? i) (+ (tref t 0) a))
      (else
       (summed t (sub1 i) (+ (tref t i) a))))))
</pre>
</div>

<p>
And this is how it behaves:
</p>

<div class="org-src-container">
<pre class="src src-racket">(sum-1 [10.0 12.0 14.0])
</pre>
</div>

<p>
which returns:
</p>

<div class="org-src-container">
<pre class="src src-racket">32
</pre>
</div>

<p>
Here's <i>sum</i> working on a tensor<sup>3</sup>:
</p>

<div class="org-src-container">
<pre class="src src-racket">(sum [[[1 2] [3 4]] [[5 6] [7 8]]])

[(sum [[1 2] [3 4]])
 (sum [[5 6] [7 8]])]

[[(sum-1 [1 2]) (sum-1 [3 4])]
 [(sum-1 [5 6]) (sum-1 [7 8])]]
</pre>
</div>

<p>
which results in:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[3 7] [11 15]]
</pre>
</div>

<p>
What can be noted about the rank of the resulting tensor is that it
is <b>one rank less than the rank of the input</b>. In this case the
input tensor was rank 3 and the output tensor is rank 2.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-orgee35afe" class="outline-5">
<h5 id="orgee35afe">Loss function</h5>
<div class="outline-text-5" id="text-orgee35afe">
<p>
The loss function, also called <b>cost</b> or <b>error</b> function (source:
<a href="https://en.wikipedia.org/wiki/Loss_function">wikipedia</a>), is a mathematical process that quantifies the error margin
between a model's prediction and the actual target value. It acts as a
guide for the learning process within a machine learning algorithm
(source: <a href="https://www.datacamp.com/tutorial/loss-function-in-machine-learning">datacamp</a>), to tell us how far we are from a well-fitted
&theta;.
</p>

<p>
A scalar is required in order to gauge how far we are from the
well-fitted &theta;, which is known as the <b>loss</b>. The loss should be as
close to 0.0 as possible.
</p>

<p>
This loss is determined every time &theta; is revised, and since the
loss shows how far we are from the well-fitted &theta;, it is used as
guide for revising &theta;.
</p>

<p>
The most straightforward way to determine how far we are is by
figuring out the difference between the given <i>ys</i> and the predicted
<i>ys</i>, by using a function as such:
</p>

<div class="org-src-container">
<pre class="src src-racket">(sum
 (- line-ys ((line line-xs) (theta-0 theta-1))))
</pre>
</div>

<p>
This however, would not be good enough as if we had a dataset and a
&theta; that gave us a tensor difference of:
</p>

<div class="org-src-container">
<pre class="src src-racket">[4.0 -3.0 0.0 -4.0 3.0]
</pre>
</div>

<p>
the result of:
</p>

<div class="org-src-container">
<pre class="src src-racket">(sum [4.0 -3.0 0.0 -4.0 3.0])
</pre>
</div>

<p>
would return an ideal loss of 0.0, which is incorrect. As can be
observed, although the individual difference are significant in most
cases, the sum of 0.0 suggests that the &theta; is a perfect fit.
</p>

<p>
This problem arises from <b>having negative values</b> in the argument to
the function <i>sum</i>. The way this can be remedied is by <b>squaring</b> each
element in the tensor difference, which turns each negative element to
positive (and the positive elements remain positive), calling the
function like this:
</p>

<div class="org-src-container">
<pre class="src src-racket">(sum
 (sqr
  (- line-ys ((line line-xs) (theta-0 theta-1)))))
</pre>
</div>

<p>
Now the sum of the squares is positive if at least one of the elements
in the tensor difference is non-zero.<br />
This function is called <b>l2-loss</b> and should <b>not</b> be confused with
<i>mean squared error</i> (or <i>MSE</i>). <i>MSE</i> would divide the sum of the
squares by the number of elements in the tensor.
</p>

<p>
One thing to note about the l2-loss function is that it can in some
case not perform well due to the presence of <a href="#org99c01cb">outliers</a>. This is
because the squaring of an element from a tensor difference which
belongs to an outlier can contribute disproportionately to the loss.
</p>
</div>
</div>
<div id="outline-container-org8b98f70" class="outline-5">
<h5 id="org8b98f70">Parameterized functions</h5>
<div class="outline-text-5" id="text-org8b98f70">
<p>
Functions such as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(define line
  (lambda (x)
    (lambda (theta)
      (+ (* (ref theta 0) x) (ref theta 1)))))
</pre>
</div>

<p>
are known as <b>parameterized functions</b>.
</p>

<p>
Parameterized functions are used where we must figure out the right
values for the parameters (here, &theta;<sub>0</sub> and &theta;<sub>1</sub>) from given
values of x and the corresponding values of y.
</p>

<p>
In this case when line is invoked, for example, with the <b>argument</b> 8
as such:
</p>

<div class="org-src-container">
<pre class="src src-racket">(line 8)
</pre>
</div>

<p>
we can say that it is waiting to accept arguments for its parameters
&theta;<sub>0</sub> and &theta;<sub>1</sub>.
</p>

<p>
Another thing to note is that when (line 8) is invoked on &theta;<sub>0</sub> and
&theta;<sub>1</sub> as such:
</p>

<div class="org-src-container">
<pre class="src src-racket">((line 8) 4 6)
</pre>
</div>

<p>
we can then determine y.
</p>
</div>
</div>
</div>
<div id="outline-container-orgde32033" class="outline-4">
<h4 id="orgde32033">Gradients</h4>
<div class="outline-text-4" id="text-orgde32033">
</div>
<div id="outline-container-org9639605" class="outline-5">
<h5 id="org9639605">Nomenclature</h5>
<div class="outline-text-5" id="text-org9639605">
<ul class="org-ul">
<li><p>
<span class="underline">Gradient short definition</span>
</p>

<p>
A gradient is a general way of understanding the rate of change of a
parameterized function with respect to <b>all</b> its parameters.
</p></li>

<li><p>
<span class="underline">Gradient fancy name</span>
</p>

<p>
The gradient is a fancy word for derivative, or the rate of change of
a function.
</p>

<p>
The term "gradient" is typically used for functions with several
inputs and a single output (a scalar field). Yes, you can say a line
has a gradient (its slope), but using "gradient" for single-variable
functions is unnecessarily confusing (source: <a href="https://betterexplained.com/articles/vector-calculus-understanding-the-gradient/">betterexplained</a>).
</p></li>

<li><p>
<span class="underline"><a id="org0ce0959"></a>&nabla; function short explanation</span>
</p>

<p>
To find the gradient of a function at given values of its arguments,
we need to use the function &nabla; (i.e. <i>gradient-of</i>). The first
argument to &nabla; is a function <i>f</i> (an <a href="#org7925b9e">objective function</a>) that
computes the <a href="#orgee35afe">loss</a> (a scalar value) using a dataset and the list of
parameters &theta; as its input. The second argument to &nabla; is the
initial values of &theta;, the parameters for which we want to
compute the gradients of the function <i>f</i>.<br />
In other words &nabla; computes the gradient of <b>another function</b>
with respect to its parameters.
</p>

<p>
The way to call &nabla; is like this:
</p>

<div class="org-src-container">
<pre class="src src-racket">(gradient-of ((l2-loss line) line-xs line-ys) (list 0.0 0.0))
</pre>
</div>

<p>
and this is a detailed explanation of how it works:
</p>

<p>
<b>Key points</b>:
</p>

<ol class="org-ol">
<li><p>
The (<a href="#org7925b9e">objective</a>) function <i>f</i> generated by
</p>

<div class="org-src-container">
<pre class="src src-racket">((l2-loss line) line-xs line-ys)
</pre>
</div>

<p>
is designed to take &theta; (<b>a list of tensors</b>) as its input.
</p></li>

<li>The &nabla; function itself manages the passing of &theta; to <i>f</i>
internally.</li>

<li>Inside &nabla;:

<ul class="org-ul">
<li><i>f</i> is invoked with <i>dual numbers</i> version of &theta;.</li>

<li>This invocation effectively passes the &theta; values to <i>f</i>.</li>
</ul></li>
</ol>

<p>
The result of the &nabla; function is a list of gradients of the
<a href="#org7925b9e">objective function</a> with respect to <b>each</b> parameter in &theta;, and
is referred to as the gradient list.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org7161a33" class="outline-5">
<h5 id="org7161a33">Gradient descent</h5>
<div class="outline-text-5" id="text-org7161a33">
<p>
Here is a graph of what is referred to (in this particular book) as
a <b>loss curve</b>:
</p>


<div id="org7f6f53b" class="figure">
<p><img src="img/LL_loss_curve.png" alt="LL_loss_curve.png" />
</p>
<p><span class="figure-number">Figure 3: </span>Loss curve</p>
</div>

<p>
The <i>y</i>-axis represents the <a href="#orgee35afe">loss</a> and the <i>x</i>-axis represents &theta;<sub>0</sub>,
which is also referred to as <b>weight</b>. So for any possible value of
the weight, this graph shows the corresponding value of the loss <b>as
an orange dot</b>.<br />
To draw this graph five weights (i.e. <b>five different values for</b>
&theta;<sub>0</sub>) were chosen, -1.0, 0.0, 1.0, 2.0, and 3.0.
</p>

<p>
For each weight, we determine its corresponding loss while keeping
&theta;<sub>1</sub> (i.e. the <b>bias</b>) at 0.0, using the following dataset:
</p>

<div class="org-src-container">
<pre class="src src-racket">[2.0 1.0 4.0 3.0]
</pre>
</div>

<p>
for the x values (<i>line-xs</i>), and
</p>

<div class="org-src-container">
<pre class="src src-racket">[1.8 1.2 4.2 3.3]
</pre>
</div>

<p>
for the y values (<i>line-ys</i>).<br />
Subsequently, this <a href="#org4444a28">objective functions</a> (<i>obj</i>) is used:
</p>

<div class="org-src-container">
<pre class="src src-racket">((l2-loss line) line-xs line-ys)
</pre>
</div>

<p>
by providing it with a &theta; which is constructed out of each of
those five weights for &theta;<sub>0</sub>, and 0.0 for &theta;<sub>1</sub> (the bias). In
other words, the <i>obj</i> expression takes parameters as its arguments
and returns a scalar which represents the loss.<br />
Those are the corresponding losses for each weight:
</p>

<div class="org-src-container">
<pre class="src src-racket">(obj (list -1.0 0.0))

=&gt; 126.21
</pre>
</div>

<div class="org-src-container">
<pre class="src src-racket">(obj (list 0.0 0.0))

=&gt; 33.21
</pre>
</div>

<div class="org-src-container">
<pre class="src src-racket">(obj (list 1.0 0.0))

=&gt; 0.21
</pre>
</div>

<div class="org-src-container">
<pre class="src src-racket">(obj (list 2.0 0.0))

=&gt; 27.21
</pre>
</div>

<div class="org-src-container">
<pre class="src src-racket">(obj (list 3.0 0.0))

=&gt; 114.21
</pre>
</div>

<p>
This type of graph is used to show <i>quantities</i> that are <b>not</b> part of
the dataset. In this context, <i>quantities</i> are values or metrics that
are derived from the model and its training process (i.e. <b>the
relationship between the loss and</b> &theta;) rather than being directly
observed data points (i.e. <b>these are not part of the raw input
data</b>).<br />
In other words, the graph illustrates internal aspects of the model
(like the parameter values and loss) rather than showing the raw input
data directly. This distinction is important because it shifts the
focus from the data itself to the model's behavior and performance
during training.
</p>

<p>
We can see on the graph that an estimate weight value of 0.0 for
&theta;<sub>0</sub> (whilst keeping &theta;<sub>1</sub> at 0.0) corresponds to a loss of
33.21. The point (0.0, 33.21) is circled in this graph:
</p>


<div id="orgb948b55" class="figure">
<p><img src="img/LL_loss_curve_2.png" alt="LL_loss_curve_2.png" />
</p>
<p><span class="figure-number">Figure 4: </span>Loss curve</p>
</div>

<p>
One can also observe that the loss is lowest at the <b>bottom</b> of the
curve. Because of this, what we need to do is to <i>roll</i> down the curve
to get to its bottom. The idea is to roll down as fast as possible
without <a href="#org9beed48">overshooting</a>, we use the <a href="#org6e82853">rate of change</a> for this.
</p>

<p>
Here's how the rate of change is represented on the loss curve:
</p>


<div id="org2dc1f56" class="figure">
<p><img src="img/LL_loss_curve_3.png" alt="LL_loss_curve_3.png" />
</p>
<p><span class="figure-number">Figure 5: </span>Loss curve with tangent</p>
</div>

<p>
The turquoise line is known as a <i>tangent</i> and touches the curve at
exactly <b>one</b> point, which in this case is (0.0 33.21). The rate of
change determined is the <i>slope</i> of the tangent, it is called the
<a href="#org9639605">gradient</a>.
</p>

<p>
Now if we were to <a href="#orgde0f8b8">revise</a> our &theta;<sub>0</sub> by, say 0.6623, this is how the
loss curve would look like:
</p>


<div id="orgfa898cb" class="figure">
<p><img src="img/LL_loss_curve_4.png" alt="LL_loss_curve_4.png" />
</p>
<p><span class="figure-number">Figure 6: </span>Loss curve with tangents</p>
</div>

<p>
What's interesting about this graph is that the second (revised)
tangent is less steep compared to the tangent for the initial &theta;<sub>0</sub>
estimate (of 0.0). Therefore, the gradient of the second is lower than
the first.
</p>

<p>
In order to find the gradient we need to use the function <a href="#org0ce0959">&nabla;</a>.
</p>
</div>
</div>
</div>
<div id="outline-container-org6e82853" class="outline-4">
<h4 id="org6e82853">Rate of change</h4>
<div class="outline-text-4" id="text-org6e82853">
<ul class="org-ul">
<li><p>
<span class="underline">Rate of change short definition</span>
</p>

<p>
The rate of change of a function (of the <a href="#org4444a28">objective</a> function in most
cases), such as:
</p>

<div class="org-src-container">
<pre class="src src-racket">((l2-loss line) line-xs line-ys)
</pre>
</div>

<p>
determines how its result changes when its argument (i.e. &theta;) is
revised.
</p>

<p>
The rate of change is also known as the <b>derivative</b>.
</p>

<p>
A more concrete example of this would be to invoke an objective
function as such:
</p>

<div class="org-src-container">
<pre class="src src-racket">(((l2-loss line) line-xs line-ys) (list 0.0 0.0))
</pre>
</div>

<p>
which would achieve the objective of finding a well-fitted &theta; by
returning the <a href="#orgee35afe">loss</a> for this particular &theta;.
</p>

<p>
If the returned loss was, for instance, 33.21, we would test the
behaviour of &theta;<sub>0</sub> to see how we should revise it. We then change
&theta;<sub>0</sub> by increasing it a small amount for testing purposes, so
that our new &theta;<sub>0</sub> is 0.0099. If the loss goes down, for
instance, to 32.59, we are slightly closer to our ideal loss. In
other words we changed the loss by:
</p>

<p>
(32.59 - 33.21) = -0.62
</p>

<p>
Now that increasing our &theta;<sub>0</sub> by 0.0099 has changed our loss by
-0.62 we would say that our <b>rate of change</b> is:
</p>

<p>
-0.62 / 0.0099 = -62.63
</p></li>

<li><p>
<span class="underline">Rate of change calculation</span>
</p>

<p>
The rate of change is determined by subtracting the old (which so far
has been greater) loss from the new (which so far has been smaller)
loss, and in our examples so far this has resulted in negative values.
</p></li>

<li><p>
<span class="underline">Using the rate of change</span>
</p>

<p>
Increasing &theta; from 0.0 by a small value can result in a rate of
change which has a large <b>absolute value</b>, meaning that a small
increase in &theta; causes a relatively large decrease in its loss.
</p>

<p>
This idea can be used to determine how much further to revise &theta;
so as to achieve a bigger loss. However we should be wary that the
revision of &theta; moves us <b>closer</b> but does not <a id="org9beed48"></a>
<b>overshoot</b> the ideal loss.
</p>

<p>
This problem can be resolved by taking a small scalar (like 0.01),
<b>and multiply the rate of change by it and revise our</b> &theta; <b>by
that amount</b>.
</p>

<p>
This small scalar is known as the <b>learning rate</b>.
</p></li>

<li><p>
<span class="underline">&theta; <a id="orgde0f8b8"></a>revision after finding the rate of change</span>
</p>

<p>
The rate of change is multiplied by alpha (the learning rate) and the
returned (negative) value used to update/revise &theta; by subtracting
from &theta; this negative value (which has resulted in a positive
revision of &theta; so far).
</p>

<p>
The rate of change <b>cannot</b> be reused as it depends on the current
&theta;.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org2d775a1" class="outline-4">
<h4 id="org2d775a1">RMSProp</h4>
<div class="outline-text-4" id="text-org2d775a1">
<ul class="org-ul">
<li><p>
<span class="underline">RMSProp short definition</span>
</p>

<p>
This algorithm works by modifying the <b>fraction</b> of the <b>gradient</b> used at
each revision.
</p>

<p>
Since our alpha so far has been a constant, we know that it causes the
velocity of the gradient descent to slow down in a similar way.
</p>

<p>
Because alpha represents the fraction of the gradient we're going to
use as our velocity, another approach to addressing this problem is to
make this fraction <b>adaptive</b>. Adaptive here means that the fraction
is decided based on the gradient and its <b>historical</b> values.
</p></li>

<li><p>
<span class="underline">RMSProp reason for squaring gradient in smooth invocation</span>
</p>

<p>
The gradient g can be negative, and if we get too many consecutive
negative gradients, then our historical averages can themselves become
negative.
</p>

<p>
This is a problem because r gets used by the <b>modifier</b>" G i.e.
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ (sqrt r) epsilon)
</pre>
</div>

<p>
and its being negative can make alpha-hat (i.e. the learning rate)
negative. When that happens, we end up ascending the gradient instead
of descending it.
</p>

<p>
This means that we would move our &theta; in a direction that
<b>increases</b> the loss instead of a direction that <b>decreases</b> the
loss.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org860c051" class="outline-4">
<h4 id="org860c051">Tensors</h4>
<div class="outline-text-4" id="text-org860c051">
<ul class="org-ul">
<li><p>
Here is a tensor<sup>1</sup>. A tensor<sup>1</sup> has only <b>scalars</b> and groups scalars
together:
</p>

<div class="org-src-container">
<pre class="src src-racket">[5.0 7.18 3.1416]
</pre>
</div></li>

<li><p>
A tensor<sup>2</sup> can be thought of as a <b>matrix</b> or a <b>two-dimensional
array</b>. The elements of a tensor<sup>2</sup> are tensors<sup>1</sup>, for example:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[7 6 2 5] [3 8 6 9] [9 4 6 5]]
</pre>
</div>

<p>
has 3 elements:
</p>

<div class="org-src-container">
<pre class="src src-racket">[7 6 2 5]
[3 8 6 9]
</pre>
</div>

<p>
and:
</p>

<div class="org-src-container">
<pre class="src src-racket">[9 4 6 5]
</pre>
</div>

<p>
Therefore if we have a tensor whose elements are tensors<sup>m</sup>, that
makes it a tensor<sup>m+1</sup>. One condition however is that all the
tensors<sup>m</sup> must have the same numbers of elements.
</p></li>

<li><p>
A scalar such as:
</p>

<div class="org-src-container">
<pre class="src src-racket">9
</pre>
</div>

<p>
is also a tensor. It is a tensor<sup>0</sup>, but <b>zero-dimensional</b> arrays
are rarely mentioned.
</p></li>
</ul>
</div>
<div id="outline-container-orge231765" class="outline-5">
<h5 id="orge231765">Tensor rank</h5>
<div class="outline-text-5" id="text-orge231765">
<ul class="org-ul">
<li><p>
The above superscripts have a name, they are known as the <b>rank</b> of
the tensor. The rank of a tensor tells us how deeply nested its
elements are. For instance, here is a tensor<sup>3</sup>:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[[8 9] [4 7]]]
</pre>
</div>

<p>
this is because it has 1 tensor<sup>2</sup> element that has 2 tensor<sup>1</sup>
elements of 2 scalars each.
</p></li>

<li><p>
In any given tensor, the nested tensors have the same number of
elements. For example, the nested tensors of tensors<sup>2</sup> are all
tensors<sup>1</sup>, and each of those tensors<sup>1</sup> has the same number of
tensors<sup>0</sup>.
</p>

<p>
This means that the tensors<sup>m</sup> that are elements of a tensor<sup>m+1</sup>
have the same <b>shape</b>.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-orgd4ebb43" class="outline-5">
<h5 id="orgd4ebb43">Tensor shape</h5>
<div class="outline-text-5" id="text-orgd4ebb43">
<ul class="org-ul">
<li><p>
The <b>shape</b> of:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[5.2 6.3 8.0] [6.9 7.1 0.5]]
</pre>
</div>

<p>
is this list of positive natural numbers:
</p>

<div class="org-src-container">
<pre class="src src-racket">(list 2 3)
</pre>
</div>

<p>
because it is a tensor<sup>2</sup> of 2 tensors<sup>1</sup>, each of which has 3
tensors<sup>0</sup> elements.
</p>

<p>
The shape of:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[[5] [6] [8]] [[7] [9] [5]]]
</pre>
</div>

<p>
is:
</p>

<div class="org-src-container">
<pre class="src src-racket">(list 2 3 1)
</pre>
</div>

<p>
It is:
</p>

<ul class="org-ul">
<li>a tensor<sup>3</sup> of 2 tensor<sup>2</sup> elements.</li>

<li>Each of those tensor<sup>2</sup> has 3 tensor<sup>1</sup> elements.</li>

<li>Each of those tensor<sup>1</sup> has 1 tensor<sup>0</sup> element, which is a scalar.</li>
</ul></li>

<li>Another useful thing to note is that the rank of a tensor is equal
to the length of its shape.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org72e1185" class="outline-4">
<h4 id="org72e1185">Update of parameters</h4>
<div class="outline-text-4" id="text-org72e1185">
<p>
The following function:
</p>

<div class="org-src-container">
<pre class="src src-racket">(define naked-u
  (lambda (P g)
    (− P (* alpha g))))
</pre>
</div>

<p>
updates the parameters by multiplying the gradient g by the learning
rate alpha, and subtracts the result from the parameter P to yield the
next P, so that ultimately we get closer to a well-fitted &theta;.
</p>
</div>
</div>
<div id="outline-container-org36681e1" class="outline-4">
<h4 id="org36681e1">Velocity</h4>
<div class="outline-text-4" id="text-org36681e1">
<ul class="org-ul">
<li><p>
<span class="underline">Velocity of descent short definition</span>
</p>

<p>
The <b>change</b> that we make to a given parameter at each revision is known
as the <b>velocity of descent</b>.
</p>

<p>
In the following expression:
</p>

<div class="org-src-container">
<pre class="src src-racket">(define naked-u
  (lambda (P g)
    (− P (* alpha g))))
</pre>
</div>

<p>
since we subtract (* alpha g), the change to P, (i.e., the velocity)
is:
</p>

<div class="org-src-container">
<pre class="src src-racket">(− (* alpha g))
</pre>
</div></li>

<li><p>
<span class="underline">Velocity of descent slowing concept</span>
</p>

<p>
We can observe from a loss graph with tangents, that as each tangent
approaches the lowest point on a graph, they get less and less steep
towards the bottom of the curve i.e. their slope (<b>the gradient</b>) gets
smaller.
</p>

<p>
In fact, as the curve's bottom is approached, the gradient gets closer
and closer to 0.0.
</p>

<p>
What happens when we multiply a really small gradient with a really
small learning rate as we do:
</p>

<div class="org-src-container">
<pre class="src src-racket">(* alpha g)
</pre>
</div>

<p>
in update functions is that we get something even smaller. So at each
revision closer to the bottom, the amount of change to each parameter
gets smaller and smaller.
</p>

<p>
Therefore we can say the <b>velocity of descent</b> slows down as we
approach the bottom of the curve.
</p></li>

<li><p>
<span class="underline">Velocity of descent speed up concept</span>
</p>

<p>
The problem of the <b>velocity of descent</b> slowing down can be remedied
by boosting our velocity. This is achieved by adding some fraction
<b>mu</b> of the velocity <b>v</b>, of the previous revision, to the change we
expect to make in the current revision.
</p>

<p>
Therefore our velocity which was:
</p>

<div class="org-src-container">
<pre class="src src-racket">(− (* alpha g))
</pre>
</div>

<p>
becomes:
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ (* mu v) (- (* alpha g)))
</pre>
</div>

<p>
which is better written as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(- (* mu v) (* alpha g))
</pre>
</div>

<p>
Here, <b>v</b> represents the velocity of the most recent revision.
</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: dm</p>
<p class="date">Created: 2024-06-01 Sat 06:56</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
