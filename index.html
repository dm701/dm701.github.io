<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-05-22 Wed 11:24 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>dm701</title>
<meta name="author" content="dm" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/js/readtheorg.js"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">dm701</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgb9cc023">1. Github repositories</a></li>
<li><a href="#org17eb1b2">2. Machine learning and deep learning</a>
<ul>
<li><a href="#org8b0eb00">2.1. General</a>
<ul>
<li><a href="#org5652949">2.1.1. Elementwise operations</a></li>
<li><a href="#org4cf7bc6">2.1.2. Hadamard product</a></li>
<li><a href="#org87f96a4">2.1.3. Tensor broadcasting</a></li>
</ul>
</li>
<li><a href="#org708d982">2.2. Little learner</a>
<ul>
<li><a href="#orgb119b63">2.2.1. Functions</a>
<ul>
<li><a href="#org4444a28">2.2.1.1. Expectant and objective functions</a></li>
<li><a href="#org38cd5df">2.2.1.2. Extended functions</a></li>
<li><a href="#org8b98f70">2.2.1.3. Parameterized functions</a></li>
</ul>
</li>
<li><a href="#orgde32033">2.2.2. Gradients</a></li>
<li><a href="#org6e82853">2.2.3. Rate of change</a></li>
<li><a href="#org2d775a1">2.2.4. RMSProp</a></li>
<li><a href="#org860c051">2.2.5. Tensors</a>
<ul>
<li><a href="#orge231765">2.2.5.1. Tensor rank</a></li>
<li><a href="#orgd4ebb43">2.2.5.2. Tensor shape</a></li>
</ul>
</li>
<li><a href="#org72e1185">2.2.6. Update of parameters</a></li>
<li><a href="#org36681e1">2.2.7. Velocity</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgb9cc023" class="outline-2">
<h2 id="orgb9cc023"><span class="section-number-2">1.</span> Github repositories</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li><a href="https://github.com/dm701/AI-for-media">AI for media</a></li>
<li><a href="https://github.com/dm701/Data-Science-assignments">Data science assignments</a></li>
<li><a href="https://github.com/dm701/NLP-project">NLP project</a></li>
<li><a href="https://github.com/dm701/Personalisation-and-machine-learning">Personalisation and machine learning</a></li>
<li><a href="https://github.com/dm701/STEM-assignments">STEM assignments</a></li>
</ul>
</div>
</div>
<div id="outline-container-org17eb1b2" class="outline-2">
<h2 id="org17eb1b2"><span class="section-number-2">2.</span> Machine learning and deep learning</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org8b0eb00" class="outline-3">
<h3 id="org8b0eb00"><span class="section-number-3">2.1.</span> General</h3>
<div class="outline-text-3" id="text-2-1">
</div>
<div id="outline-container-org5652949" class="outline-4">
<h4 id="org5652949"><span class="section-number-4">2.1.1.</span> Elementwise operations</h4>
<div class="outline-text-4" id="text-2-1-1">
<p>
In elementwise operations like addition, subtraction, and division,
values that correspond positionally are combined to produce a new
tensor. The first value in tensor <i>A</i> is paired with the first value
in tensor <i>B</i>. The second value is paired with the second, and so
on. This means the tensors must have equal dimensions (i.e. equal
<a href="#orgd4ebb43">shapes</a>) in order to complete the operation.<br />
</p>


<div id="org789b34d" class="figure">
<p><img src="img/Elementwise_operations.png" alt="Elementwise_operations.png" />
</p>
<p><span class="figure-number">Figure 1: </span>Elementwise operation</p>
</div>

<p>
Source: <a href="https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#elementwise-operations">ML-cheatsheets</a>
</p>
</div>
</div>
<div id="outline-container-org4cf7bc6" class="outline-4">
<h4 id="org4cf7bc6"><span class="section-number-4">2.1.2.</span> Hadamard product</h4>
<div class="outline-text-4" id="text-2-1-2">
<p>
According to <a href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices)">wikipedia</a>, the <b>Hadamard product</b> (also known as <b>element
wise product</b>) is an operation which takes in two matrices of the same
shape and returns a matrix of the multiplied corresponding elements.
</p>
</div>
</div>
<div id="outline-container-org87f96a4" class="outline-4">
<h4 id="org87f96a4"><span class="section-number-4">2.1.3.</span> Tensor broadcasting</h4>
<div class="outline-text-4" id="text-2-1-3">
<p>
According to <a href="https://numpy.org/doc/stable/user/basics.broadcasting.html">numpy</a>, tensor operations are usually done on pairs of
tensors on an element-by-element basis. In the simplest case, the two
tensors must have the exact same shape. For example:
</p>

<div class="org-src-container">
<pre class="src src-racket">(* [1.0 2.0 3.0] [2.0 2.0 2.0])
</pre>
</div>

<p>
will return:
</p>

<div class="org-src-container">
<pre class="src src-racket">[2.0 4.0 6.0]
</pre>
</div>

<p>
Broadcasting rules can <b>relax</b> these constraints when the tensors'
shapes meet certain constraints. The simplest broadcasting example
occurs when a tensor and a scalar are combined in an operation:
</p>

<div class="org-src-container">
<pre class="src src-racket">(* [1.0 2.0 3.0] 2.0)
</pre>
</div>

<p>
returns:
</p>

<div class="org-src-container">
<pre class="src src-racket">[2.0 4.0 6.0]
</pre>
</div>

<p>
We can think of the scalar being <b>stretched</b> during the arithmetic
operation, into a tensor the same shape as the first one. The new
elements in the stretched tensor are just copies of the original
scalar.
</p>


<div id="orgea804e7" class="figure">
<p><img src="img/broadcasting_1.png" alt="broadcasting_1.png" />
</p>
<p><span class="figure-number">Figure 2: </span>In the simplest example of broadcasting, the scalar <b>b</b> is stretched to become an array of same shape as <b>a</b> so the shapes are compatible for element-by-element multiplication. Source <a href="https://numpy.org/doc/stable/user/basics.broadcasting.html">Numpy</a></p>
</div>
</div>
</div>
</div>
<div id="outline-container-org708d982" class="outline-3">
<h3 id="org708d982"><span class="section-number-3">2.2.</span> Little learner</h3>
<div class="outline-text-3" id="text-2-2">
<p>
This section is a series of notes about machine learning and deep
learning in the context of the book <a href="https://www.thelittlelearner.com/">"The Little Learner: A straight
line to deep learning"</a> by <i>Daniel P. Friedman and Anurag Mendhekar</i>.
</p>
</div>
<div id="outline-container-orgb119b63" class="outline-4">
<h4 id="orgb119b63"><span class="section-number-4">2.2.1.</span> Functions</h4>
<div class="outline-text-4" id="text-2-2-1">
</div>
<div id="outline-container-org4444a28" class="outline-5">
<h5 id="org4444a28"><span class="section-number-5">2.2.1.1.</span> Expectant and objective functions</h5>
<div class="outline-text-5" id="text-2-2-1-1">
<p>
A function invocation such as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(l2-loss line)
</pre>
</div>

<p>
which in a "same-as" chart transcribes as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(lambda (xs ys)
  (lambda (theta)
    (let ((pred-ys ((line xs) theta)))
      (sum
       (sqr
        (- ys pred-ys)))))
</pre>
</div>

<p>
produces another function. This function which is produced when
<b>l2-loss</b> is invoked with a <b>target</b> function (<b>line</b> in this case),
is referred to as an <b>expectant</b> function. This is because it is
<b>expecting</b> a data set as arguments.
</p>

<p>
When an expectant function receives a data set, for example:
</p>

<div class="org-src-container">
<pre class="src src-racket">((l2-loss line) line-xs line-ys)
</pre>
</div>

<p>
which in a "same-as" chart transcribes as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(lambda (theta)
  (let ((pred-ys ((line line-xs) theta)))
    (sum
     (sqr
      (- line-ys pred-ys)))))
</pre>
</div>

<p>
produces a function which <b>awaits</b> a theta. The name of the produced
function is known as an <b>objective</b> function. When provided with a
theta, the objective function returns a scalar representing the
<b>loss</b>, which is a measure of how far away we are from the well fitted
theta.
</p>

<p>
The objective function would be called as such:
</p>

<div class="org-src-container">
<pre class="src src-racket">(((l2-loss line) line-xs line-ys) (list 0.0 0.0))
</pre>
</div>
</div>
</div>
<div id="outline-container-org38cd5df" class="outline-5">
<h5 id="org38cd5df"><span class="section-number-5">2.2.1.2.</span> Extended functions</h5>
<div class="outline-text-5" id="text-2-2-1-2">
<ul class="org-ul">
<li><p>
In the context of this book, some functions were built to work on
both scalars and tensors. They are referred as <b>extended</b>
functions.<br />
For instance, the function:
</p>

<div class="org-src-container">
<pre class="src src-racket">+
</pre>
</div>

<p>
is one of them.<br />
Other functions that work with scalars can be extended similarly.
</p></li>

<li><p>
In its "regular" (i.e. non-extended) way, the function + works as
such:
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ 1 1)
5
</pre>
</div>

<p>
However, because + is built using <b>extension</b> it can also work on
tensors:
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ [2] [7])
[9]
</pre>
</div>

<p>
here is a "same-as" chart to illustrate how this results in [9]:
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ [2] [7])
[(+ 2 7)]
[9]
</pre>
</div>

<p>
When there is a function invocation like + on tensors, we look
inside those tensors to determine the invocation's final value.
</p>

<p>
Here is another example:
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ [5 6 7] [2 0 1])
</pre>
</div>

<p>
transcribes as:
</p>

<div class="org-src-container">
<pre class="src src-racket">[(+ 5 2) (+ 6 0) (+ 7 1)]
</pre>
</div>

<p>
and results in:
</p>

<div class="org-src-container">
<pre class="src src-racket">[7 6 8]
</pre>
</div>

<p>
It's as if + <b>descends</b> into its tensor<sup>1</sup> arguments to results in
another tensor<sup>1</sup>. The last step results in the tensor<sup>1</sup> of the
values of the three sums.
</p>

<p>
We can also add 2 tensors<sup>2</sup> of the same shape, for instance:
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ [[4 6 7] [2 0 1]]
   [[1 2 2] [6 3 1]])
</pre>
</div>

<p>
transcribes as:
</p>

<div class="org-src-container">
<pre class="src src-racket">[(+ [4 6 7] [1 2 2])
 (+ [2 0 1] [6 3 1])]

[[(+ 4 1) (+ 6 2) (+ 7 2)]
 [(+ 2 6) (+ 0 3) (+ 1 1)]]
</pre>
</div>

<p>
and results in:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[5 8 9] [8 3 2]]
</pre>
</div>

<p>
The authors then go on to explain that tensors <b>must</b> be of the same
shape before they can be added together, and that getting functions
such as + to work on tensors of <b>arbitrary</b> ranks is known as
<b>pointwise extension</b>. This however seems to be more commonly known
as <a href="#org5652949">elementwise operations</a>.
</p></li>

<li><p>
The point by the authors that tensors <b>must</b> have the same shape
before computation can be performed on them is, later in the text,
contradicted and unfortunately not very well explained. Nevertheless
we are shown that the following operation can actually be performed
on tensors:
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ 4 [3 6 5])
</pre>
</div>

<p>
by doing this:
</p>

<div class="org-src-container">
<pre class="src src-racket">[(+ 4 3) (+ 4 6) (+ 4 5)]
</pre>
</div>

<p>
which results in:
</p>

<div class="org-src-container">
<pre class="src src-racket">[7 10 9]
</pre>
</div>

<p>
Another example given is:
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ [6 9 1] [[4 3 8] [7 4 7]])
</pre>
</div>

<p>
and in this case, we can look inside the tensor<sup>2</sup> argument and add
the tensor<sup>1</sup> argument just as we did in the previous addition:
</p>

<div class="org-src-container">
<pre class="src src-racket">[(+ [6 9 1] [4 3 8])
 (+ [6 9 1] [7 4 7])]

[[(+ 6 4) (+ 9 3) (+ 1 8)]
 [(+ 6 7) (+ 9 4) (+ 1 7)]]
</pre>
</div>

<p>
which results in:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[10 12 9] [13 13 8]]
</pre>
</div>

<p>
This is in fact called <a href="#org87f96a4">tensor broadcasting</a>.
</p></li>

<li><p>
The reader is subsequently introduced to the extended version of:
</p>

<div class="org-src-container">
<pre class="src src-racket">*
</pre>
</div>

<p>
with the following operation:
</p>

<div class="org-src-container">
<pre class="src src-racket">(* [[4 6 5] [6 9 7]] 3)
</pre>
</div>

<p>
and these are the steps:
</p>

<div class="org-src-container">
<pre class="src src-racket">[(* [4 6 5] 3) (* [6 9 7] 3)]

[[(* 4 3) (* 6 3) (* 5 3)]
 [(* 6 3) (* 9 3) (* 7 3)]]
</pre>
</div>

<p>
which returns:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[12 18 15] [18 27 14]]
</pre>
</div>

<p>
We are told that this is the <b>Hadamard multiplication</b>.<br />
Upon further research this does not appear to fit the definition of
the Hadamard multiplication, or rather <a href="#org4cf7bc6">Hadamard product</a>, but a
simple tensor to scalar multiplication which involves broadcasting.
</p></li>

<li><p>
Other extended functions such as:
</p>

<div class="org-src-container">
<pre class="src src-racket">sqrt
</pre>
</div>

<p>
descend into tensors. In the case of a tensor<sup>1</sup>, this is how sqrt
works:
</p>

<div class="org-src-container">
<pre class="src src-racket">(sqrt [9 16 25])

[(sqrt 9) (sqrt 16) (sqrt 25)]
</pre>
</div>

<p>
which returns:
</p>

<div class="org-src-container">
<pre class="src src-racket">[3 4 5]
</pre>
</div>

<p>
In the case of a tensor<sup>2</sup>, it works this way:
</p>

<div class="org-src-container">
<pre class="src src-racket">(sqrt [[49 81 16] [64 25 36]])

[(sqrt [[49 81 16]]) (sqrt [[64 25 36]])]

[[(sqrt 49) (sqrt 81) (sqrt 16)]
 [(sqrt 64) (sqrt 25) (sqrt 36)]]
</pre>
</div>

<p>
and this results in:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[7 9 4] [8 5 6]]
</pre>
</div>

<p>
In other words, the function descends inside each tensor<sup>1</sup> until it
finds a tensor<sup>0</sup> at which point it gets their square root.<br />
However, not all extended functions descend until they find scalars.
The function
</p>

<div class="org-src-container">
<pre class="src src-racket">sum
</pre>
</div>

<p>
which extends the function
</p>

<div class="org-src-container">
<pre class="src src-racket">sum-1
</pre>
</div>

<p>
descends into its argument until it finds a tensor<sup>1</sup> instead of a
tensor<sup>0</sup>.<br />
This is how sum-1 is defined:
</p>

<div class="org-src-container">
<pre class="src src-racket">(define sum1
  (lambda (t)
    (summed t (sub1 ) 0.0)))

(define summed
  (lambda (t i a)
    (cond
      ((zero? i) (+ (tref t 0) a))
      (else
       (summed t (sub1 i) (+ (tref t i) a))))))
</pre>
</div>

<p>
And this is how it behaves:
</p>

<div class="org-src-container">
<pre class="src src-racket">(sum-1 [10.0 12.0 14.0])
</pre>
</div>

<p>
which returns:
</p>

<div class="org-src-container">
<pre class="src src-racket">32
</pre>
</div>

<p>
Here's <i>sum</i> working on a tensor<sup>3</sup>:
</p>

<div class="org-src-container">
<pre class="src src-racket">(sum [[[1 2] [3 4]] [[5 6] [7 8]]])

[(sum [[1 2] [3 4]])
 (sum [[5 6] [7 8]])]

[[(sum-1 [1 2]) (sum-1 [3 4])]
 [(sum-1 [5 6]) (sum-1 [7 8])]]
</pre>
</div>

<p>
which results in:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[3 7] [11 15]]
</pre>
</div>

<p>
What can be noted about the rank of the resulting tensor is that it
is <b>one rank less than the rank of the input</b>. In this case the
input tensor was rank 3 and the output tensor is rank 2.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org8b98f70" class="outline-5">
<h5 id="org8b98f70"><span class="section-number-5">2.2.1.3.</span> Parameterized functions</h5>
<div class="outline-text-5" id="text-2-2-1-3">
<p>
Functions such as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(define line
  (lambda (x)
    (lambda (theta)
      (+ (* (ref theta 0) x) (ref theta 1)))))
</pre>
</div>

<p>
are known as <b>parameterized functions</b>.
</p>

<p>
Parameterized functions are used where we must figure out the right
values for the parameters (here, theta 0 and theta 1) from given
values of x and the corresponding values of y.
</p>

<p>
In this case when line is invoked, for example, with the <b>argument</b> 8
as such:
</p>

<div class="org-src-container">
<pre class="src src-racket">(line 8)
</pre>
</div>

<p>
we can say that it is waiting to accept arguments for its parameters
<b>theta 0</b> and <b>theta 1</b>.
</p>

<p>
Another thing to note is that when (line 8) is invoked on theta 0 and
theta 1 as such:
</p>

<div class="org-src-container">
<pre class="src src-racket">((line 8) 4 6)
</pre>
</div>

<p>
we can then determine y.
</p>
</div>
</div>
</div>
<div id="outline-container-orgde32033" class="outline-4">
<h4 id="orgde32033"><span class="section-number-4">2.2.2.</span> Gradients</h4>
<div class="outline-text-4" id="text-2-2-2">
<ul class="org-ul">
<li><p>
<span class="underline">Gradient short definition</span>
</p>

<p>
A gradient is a general way of understanding the rate of change of a
parameterized function with respect to <b>all</b> its parameters.
</p></li>

<li><p>
<span class="underline">Gradient fancy name</span>
</p>

<p>
The gradient is a fancy word for derivative, or the rate of change of
a function.
</p>

<p>
The term "gradient" is typically used for functions with several
inputs and a single output (a scalar field). Yes, you can say a line
has a gradient (its slope), but using "gradient" for single-variable
functions is unnecessarily confusing.
</p></li>

<li><p>
<span class="underline">Gradient-of function short explanation</span>
</p>

<p>
The result of the "gradient-of" function is a list of gradients of the
<b>objective function</b> f with respect to each parameter in theta, and is
referred to as the gradient list.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org6e82853" class="outline-4">
<h4 id="org6e82853"><span class="section-number-4">2.2.3.</span> Rate of change</h4>
<div class="outline-text-4" id="text-2-2-3">
<ul class="org-ul">
<li><p>
<span class="underline">Rate of change short definition</span>
</p>

<p>
The rate of change of a function (of the <b>objective</b> function in most
cases), such as:
</p>

<div class="org-src-container">
<pre class="src src-racket">((l2-loss line) line-xs line-ys)
</pre>
</div>

<p>
determines how its result changes when its argument (i.e. theta) is
revised.
</p>

<p>
The rate of change is also known as the <b>derivative</b>.
</p>

<p>
A more concrete example of this would be to invoke an objective
function as such:
</p>

<div class="org-src-container">
<pre class="src src-racket">(((l2-loss line) line-xs line-ys) (list 0.0 0.0))
</pre>
</div>

<p>
which would achieve the objective of finding a well-fitted theta by
returning the <b>loss</b> for this particular theta.
</p>

<p>
If the returned loss was, for instance, 33.21, we would test the
behaviour of "theta 0" to see how we should revise it. We then change
"theta 0" by increasing it a small amount for testing purposes, so
that our new "theta 0 is" 0.0099. If the loss goes down, for instance,
to 32.59, we are slightly closer to our ideal loss. In other words we
changed the loss by:
</p>

<p>
(32.59 - 33.21) = -0.62
</p>

<p>
Now that increasing our "theta 0" by 0.0099 has changed our loss by
-0.62 we would say that our <b>rate of change</b> is:
</p>

<p>
-0.62 / 0.0099 = -62.63
</p></li>

<li><p>
<span class="underline">Rate of change calculation</span>
</p>

<p>
The rate of change is determined by subtracting the old (which so far
has been greater) loss from the new (which so far has been smaller)
loss, and in our examples so far this has resulted in negative values.
</p></li>

<li><p>
<span class="underline">Using the rate of change</span>
</p>

<p>
Increasing theta from 0.0 by a small value can result in a rate of
change which has a large <b>absolute value</b>, meaning that a small
increase in theta causes a relatively large decrease in its loss.
</p>

<p>
This idea can be used to determine how much further to revise theta so
as to achieve a bigger loss. However we should be wary that the
revision of theta moves us <b>closer</b> but does not <b>overshoot</b> the ideal
loss.
</p>

<p>
This problem can be resolved by taking a small scalar (like 0.01),
<b>and multiply the rate of change by it and revise our theta by that amount</b>.
</p>

<p>
This small scalar is known as the <b>learning rate</b>.
</p></li>

<li><p>
<span class="underline">Theta revision after finding the rate of change</span>
</p>

<p>
The rate of change is multiplied by alpha (the learning rate) and the
returned (negative) value used to update/revise theta by subtracting
from theta this negative value (which has resulted in a positive
revision of theta so far).
</p>

<p>
The rate of change <b>cannot</b> be reused as it depends on the current
theta.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org2d775a1" class="outline-4">
<h4 id="org2d775a1"><span class="section-number-4">2.2.4.</span> RMSProp</h4>
<div class="outline-text-4" id="text-2-2-4">
<ul class="org-ul">
<li><p>
<span class="underline">RMSProp short definition</span>
</p>

<p>
This algorithm works by modifying the <b>fraction</b> of the <b>gradient</b> used at
each revision.
</p>

<p>
Since our alpha so far has been a constant, we know that it causes the
velocity of the gradient descent to slow down in a similar way.
</p>

<p>
Because alpha represents the fraction of the gradient we're going to
use as our velocity, another approach to addressing this problem is to
make this fraction <b>adaptive</b>. Adaptive here means that the fraction
is decided based on the gradient and its <b>historical</b> values.
</p></li>

<li><p>
<span class="underline">RMSProp reason for squaring gradient in smooth invocation</span>
The gradient g can be negative, and if we get too many consecutive
negative gradients, then our historical averages can themselves become
negative.
</p>

<p>
This is a problem because r gets used by the <b>modifier</b>" G i.e.
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ (sqrt r) epsilon)
</pre>
</div>

<p>
and its being negative can make alpha-hat (i.e. the learning rate)
negative. When that happens, we end up ascending the gradient instead
of descending it.
</p>

<p>
This means that we would move our theta in a direction that
<b>increases</b> the loss instead of a direction that <b>decreases</b> the loss.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org860c051" class="outline-4">
<h4 id="org860c051"><span class="section-number-4">2.2.5.</span> Tensors</h4>
<div class="outline-text-4" id="text-2-2-5">
<ul class="org-ul">
<li><p>
Here is a tensor<sup>1</sup>. A tensor<sup>1</sup> has only <b>scalars</b> and groups scalars
together:
</p>

<div class="org-src-container">
<pre class="src src-racket">[5.0 7.18 3.1416]
</pre>
</div></li>

<li><p>
A tensor<sup>2</sup> can be thought of as a <b>matrix</b> or a <b>two-dimensional
array</b>. The elements of a tensor<sup>2</sup> are tensors<sup>1</sup>, for example:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[7 6 2 5] [3 8 6 9] [9 4 6 5]]
</pre>
</div>

<p>
has 3 elements:
</p>

<div class="org-src-container">
<pre class="src src-racket">[7 6 2 5]
[3 8 6 9]
</pre>
</div>

<p>
and:
</p>

<div class="org-src-container">
<pre class="src src-racket">[9 4 6 5]
</pre>
</div>

<p>
Therefore if we have a tensor whose elements are tensors<sup>m</sup>, that
makes it a tensor<sup>m+1</sup>. One condition however is that all the
tensors<sup>m</sup> must have the same numbers of elements.
</p></li>

<li><p>
A scalar such as:
</p>

<div class="org-src-container">
<pre class="src src-racket">9
</pre>
</div>

<p>
is also a tensor. It is a tensor<sup>0</sup>, but <b>zero-dimensional</b> arrays
are rarely mentioned.
</p></li>
</ul>
</div>
<div id="outline-container-orge231765" class="outline-5">
<h5 id="orge231765"><span class="section-number-5">2.2.5.1.</span> Tensor rank</h5>
<div class="outline-text-5" id="text-2-2-5-1">
<ul class="org-ul">
<li><p>
The above superscripts have a name, they are known as the <b>rank</b> of
the tensor. The rank of a tensor tells us how deeply nested its
elements are. For instance, here is a tensor<sup>3</sup>:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[[8 9] [4 7]]]
</pre>
</div>

<p>
this is because it has 1 tensor<sup>2</sup> element that has 2 tensor<sup>1</sup>
elements of 2 scalars each.
</p></li>

<li><p>
In any given tensor, the nested tensors have the same number of
elements. For example, the nested tensors of tensors<sup>2</sup> are all
tensors<sup>1</sup>, and each of those tensors<sup>1</sup> has the same number of
tensors<sup>0</sup>.
</p>

<p>
This means that the tensors<sup>m</sup> that are elements of a tensor<sup>m+1</sup>
have the same <b>shape</b>.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-orgd4ebb43" class="outline-5">
<h5 id="orgd4ebb43"><span class="section-number-5">2.2.5.2.</span> Tensor shape</h5>
<div class="outline-text-5" id="text-2-2-5-2">
<ul class="org-ul">
<li><p>
The <b>shape</b> of:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[5.2 6.3 8.0] [6.9 7.1 0.5]]
</pre>
</div>

<p>
is this list of positive natural numbers:
</p>

<div class="org-src-container">
<pre class="src src-racket">(list 2 3)
</pre>
</div>

<p>
because it is a tensor<sup>2</sup> of 2 tensors<sup>1</sup>, each of which has 3
tensors<sup>0</sup> elements.
</p>

<p>
The shape of:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[[5] [6] [8]] [[7] [9] [5]]]
</pre>
</div>

<p>
is:
</p>

<div class="org-src-container">
<pre class="src src-racket">(list 2 3 1)
</pre>
</div>

<p>
It is:
</p>

<ul class="org-ul">
<li>a tensor<sup>3</sup> of 2 tensor<sup>2</sup> elements.</li>

<li>Each of those tensor<sup>2</sup> has 3 tensor<sup>1</sup> elements.</li>

<li>Each of those tensor<sup>1</sup> has 1 tensor<sup>0</sup> element, which is a scalar.</li>
</ul></li>

<li>Another useful thing to note is that the rank of a tensor is equal
to the length of its shape.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org72e1185" class="outline-4">
<h4 id="org72e1185"><span class="section-number-4">2.2.6.</span> Update of parameters</h4>
<div class="outline-text-4" id="text-2-2-6">
<p>
The following function:
</p>

<div class="org-src-container">
<pre class="src src-racket">(define naked-u
  (lambda (P g)
    (− P (* alpha g))))
</pre>
</div>

<p>
updates the parameters by multiplying the gradient g by the learning
rate alpha, and subtracts the result from the parameter P to yield the
next P, so that ultimately we get closer to a well-fitted theta.
</p>
</div>
</div>
<div id="outline-container-org36681e1" class="outline-4">
<h4 id="org36681e1"><span class="section-number-4">2.2.7.</span> Velocity</h4>
<div class="outline-text-4" id="text-2-2-7">
<ul class="org-ul">
<li><p>
<span class="underline">Velocity of descent short definition</span>
</p>

<p>
The <b>change</b> that we make to a given parameter at each revision is known
as the <b>velocity of descent</b>.
</p>

<p>
In the following expression:
</p>

<div class="org-src-container">
<pre class="src src-racket">(define naked-u
  (lambda (P g)
    (− P (* alpha g))))
</pre>
</div>

<p>
since we subtract (* alpha g), the change to P, (i.e., the velocity)
is:
</p>

<div class="org-src-container">
<pre class="src src-racket">(− (* alpha g))
</pre>
</div></li>

<li><p>
<span class="underline">Velocity of descent slowing concept</span>
</p>

<p>
We can observe from a loss graph with tangents, that as each tangent
approaches the lowest point on a graph, they get less and less steep
towards the bottom of the curve i.e. their slope (<b>the gradient</b>) gets
smaller.
</p>

<p>
In fact, as the curve's bottom is approached, the gradient gets closer
and closer to 0.0.
</p>

<p>
What happens when we multiply a really small gradient with a really
small learning rate as we do:
</p>

<div class="org-src-container">
<pre class="src src-racket">(* alpha g)
</pre>
</div>

<p>
in update functions is that we get something even smaller. So at each
revision closer to the bottom, the amount of change to each parameter
gets smaller and smaller.
</p>

<p>
Therefore we can say the <b>velocity of descent</b> slows down as we
approach the bottom of the curve.
</p></li>

<li><p>
<span class="underline">Velocity of descent speed up concept</span>
</p>

<p>
The problem of the <b>velocity of descent</b> slowing down can be remedied
by boosting our velocity. This is achieved by adding some fraction
<b>mu</b> of the velocity <b>v</b>, of the previous revision, to the change we
expect to make in the current revision.
</p>

<p>
Therefore our velocity which was:
</p>

<div class="org-src-container">
<pre class="src src-racket">(− (* alpha g))
</pre>
</div>

<p>
becomes:
</p>

<div class="org-src-container">
<pre class="src src-racket">(+ (* mu v) (- (* alpha g)))
</pre>
</div>

<p>
which is better written as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(- (* mu v) (* alpha g))
</pre>
</div>

<p>
Here, <b>v</b> represents the velocity of the most recent revision.
</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: dm</p>
<p class="date">Created: 2024-05-22 Wed 11:24</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
