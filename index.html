<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-06-22 Sat 08:42 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>dm701</title>
<meta name="author" content="dm" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/js/readtheorg.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">dm701</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orga7870df">Github repositories</a></li>
<li><a href="#org7207ef3">Machine learning and deep learning</a>
<ul>
<li><a href="#org0cd027d">General</a>
<ul>
<li><a href="#org23d7172"><span class="todo TODO">TODO</span> Automatic differentiation</a></li>
<li><a href="#org96b40d0"><span class="todo TODO">TODO</span> Complex numbers</a></li>
<li><a href="#org33885ea"><span class="todo TODO">TODO</span> Dual numbers</a></li>
<li><a href="#orgcbd8c67">Elementwise operations</a></li>
<li><a href="#org6f349a0">Hadamard product</a></li>
<li><a href="#orgf542efa"><span class="todo TODO">TODO</span> Imaginary numbers</a></li>
<li><a href="#org4a10078">Non-linear functions</a></li>
<li><a href="#org935e858">Outliers</a></li>
<li><a href="#orgf20a51b">Quadratic equations</a>
<ul>
<li><a href="#org50e3b48">Hidden quadratic equations</a></li>
<li><a href="#org270c666">Solving quadratic equations</a></li>
<li><a href="#org293e665">Using the quadratic formula</a>
<ul>
<li><a href="#orga28b2c2">Complex solutions</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgdb2c2c8">Tensor broadcasting</a></li>
</ul>
</li>
<li><a href="#orgc578a0d">Little learner</a>
<ul>
<li><a href="#org0321f62">Basic machine learning</a></li>
<li><a href="#orgc631602">Functions</a>
<ul>
<li><a href="#orgf252d17">Expectant and objective functions</a></li>
<li><a href="#org9481b3f">Extended functions</a></li>
<li><a href="#orgd426ad4">Loss function</a></li>
<li><a href="#org39437b3">Parameterized functions</a></li>
</ul>
</li>
<li><a href="#orgd131526">Gradients</a>
<ul>
<li><a href="#orge69378d">Nomenclature</a></li>
<li><a href="#org157580b">Gradient descent</a></li>
<li><a href="#orgf4a1ecc">Optimizers</a>
<ul>
<li><a href="#orgef1c387">Stochastic gradient descent</a></li>
<li><a href="#orgad48faf">Momentum gradient descent</a></li>
<li><a href="#org9596294">RMSProp</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org547febf">Rate of change</a></li>
<li><a href="#org626d160">Smoothing</a></li>
<li><a href="#org6ba01ac">Tensors</a>
<ul>
<li><a href="#org83b62ec">Tensor rank</a></li>
<li><a href="#orgc5f34e0">Tensor shape</a></li>
</ul>
</li>
<li><a href="#org50893a4">Update of parameters</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-orga7870df" class="outline-2">
<h2 id="orga7870df">Github repositories</h2>
<div class="outline-text-2" id="text-orga7870df">
<ul class="org-ul">
<li><a href="https://github.com/dm701/AI-for-media">AI for media</a></li>
<li><a href="https://github.com/dm701/Data-Science-assignments">Data science assignments</a></li>
<li><a href="https://github.com/dm701/NLP-project">NLP project</a></li>
<li><a href="https://github.com/dm701/Personalisation-and-machine-learning">Personalisation and machine learning</a></li>
<li><a href="https://github.com/dm701/STEM-assignments">STEM assignments</a></li>
</ul>
</div>
</div>
<div id="outline-container-org7207ef3" class="outline-2">
<h2 id="org7207ef3">Machine learning and deep learning</h2>
<div class="outline-text-2" id="text-org7207ef3">
</div>
<div id="outline-container-org0cd027d" class="outline-3">
<h3 id="org0cd027d">General</h3>
<div class="outline-text-3" id="text-org0cd027d">
</div>
<div id="outline-container-org23d7172" class="outline-4">
<h4 id="org23d7172"><span class="todo TODO">TODO</span> Automatic differentiation</h4>
</div>
<div id="outline-container-org96b40d0" class="outline-4">
<h4 id="org96b40d0"><span class="todo TODO">TODO</span> Complex numbers</h4>
</div>
<div id="outline-container-org33885ea" class="outline-4">
<h4 id="org33885ea"><span class="todo TODO">TODO</span> <a id="org5490c99"></a>Dual numbers</h4>
</div>
<div id="outline-container-orgcbd8c67" class="outline-4">
<h4 id="orgcbd8c67">Elementwise operations</h4>
<div class="outline-text-4" id="text-orgcbd8c67">
<p>
In elementwise operations like addition, subtraction, and division, values that correspond positionally are combined to produce a new tensor. The first value in tensor <i>A</i> is paired with the first value in tensor <i>B</i>. The second value is paired with the second, and so on. This means the tensors must have equal dimensions (i.e. equal <a href="#orgc5f34e0">shapes</a>) in order to complete the operation (source: <a href="https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#elementwise-operations">ml-cheatsheets</a>).
</p>


<div id="org7d75a58" class="figure">
<p><img src="img/Elementwise_operations.png" alt="Elementwise_operations.png" />
</p>
<p><span class="figure-number">Figure 1: </span>Elementwise operation</p>
</div>
</div>
</div>
<div id="outline-container-org6f349a0" class="outline-4">
<h4 id="org6f349a0">Hadamard product</h4>
<div class="outline-text-4" id="text-org6f349a0">
<p>
The <b>Hadamard product</b> (also known as <b>element wise product</b>) is an operation which takes in two matrices of the same shape and returns a matrix of the multiplied corresponding elements (source: <a href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices)">wikipedia</a>).
</p>
</div>
</div>
<div id="outline-container-orgf542efa" class="outline-4">
<h4 id="orgf542efa"><span class="todo TODO">TODO</span> Imaginary numbers</h4>
</div>
<div id="outline-container-org4a10078" class="outline-4">
<h4 id="org4a10078"><a id="orgad31dd3"></a>Non-linear functions</h4>
<div class="outline-text-4" id="text-org4a10078">
<p>
Often a <a href="#orgdc9ba80">linear</a> function cannot explain the relationship between variables, in such cases a non-linear function must be used.<br />
Non-linear means the graph is not a straight line, therefore the graph of a non-linear function is a curved line.
</p>

<p>
The slope of a linear function is the same no matter where on the line it is measured, by contrast the slope of a non-linear function is different at each point on the line. Thus there is no single slope for a non-linear function. However the slope can be determined at any point on the line.  The techniques of differential calculus are used to determine the slopes of non-linear functions.<br />
Examples of non-linear functions are:
</p>

<ul class="org-ul">
<li>Exponential functions</li>
<li><a href="#org431fada">Quadratic</a> functions</li>
<li>Logarithmic functions</li>
</ul>

<p>
(source: <a href="http://www.columbia.edu/itc/sipa/math/nonlinear.html">columbia.edu</a>)
</p>
</div>
</div>
<div id="outline-container-org935e858" class="outline-4">
<h4 id="org935e858">Outliers</h4>
<div class="outline-text-4" id="text-org935e858">
<p>
An outlier is a data point significantly different from other data points in a dataset. Outliers can occur for various reasons, such as measurement errors, data entry errors, or natural variations in the data. They can significantly impact analysis in machine learning.
</p>

<p>
An example of this is if we were to take the following dataset:
</p>

<div class="org-src-container">
<pre class="src src-racket">[<span style="color: #a9a1e1;">15</span> <span style="color: #a9a1e1;">101</span> <span style="color: #a9a1e1;">18</span> <span style="color: #a9a1e1;">7</span> <span style="color: #a9a1e1;">13</span> <span style="color: #a9a1e1;">16</span> <span style="color: #a9a1e1;">11</span> <span style="color: #a9a1e1;">21</span> <span style="color: #a9a1e1;">5</span> <span style="color: #a9a1e1;">15</span> <span style="color: #a9a1e1;">10</span> <span style="color: #a9a1e1;">9</span>]
</pre>
</div>

<p>
just by looking at it, one can quickly say ‘101’ is an outlier because it is much larger than the other values (source: <a href="https://www.almabetter.com/bytes/articles/outlier-detection-methods-and-techniques-in-machine-learning-with-examples">almabetter</a> and <a href="https://www.analyticsvidhya.com/blog/2021/05/detecting-and-treating-outliers-treating-the-odd-one-out/">analyticsvidhya</a>).
</p>
</div>
</div>
<div id="outline-container-orgf20a51b" class="outline-4">
<h4 id="orgf20a51b"><a id="org431fada"></a>Quadratic equations</h4>
<div class="outline-text-4" id="text-orgf20a51b">
<p>
Here's an example of a <b>standard</b> <i>quadratic equation</i>:
</p>

<p>
\[5x^2+3x+3=0\]
</p>

<p>
The name <i>quadratic</i> comes from <i>quad</i> meaning square, <b>because the variable gets squared</b> (\(x^2\)).<br />
The standard form of a quadratic equation looks like this:
</p>

<p>
\[ax^2+bx+c=0\]
</p>

<ul class="org-ul">
<li>\(a\), \(b\) and \(c\) are known values. \(a\) can't be 0.</li>
<li>\(x\) is the variable (the value which don't know yet).</li>
</ul>

<p>
Examples:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">\(2x^2+5x+3=0\)</td>
<td class="org-left">In this one \(a=2\), \(b=5\) and \(c=3\).</td>
</tr>

<tr>
<td class="org-left">\(x^2-3=0\)</td>
<td class="org-left">\(a=1\), \(1x^2\) is not normally written. $b=-3, and \(c=0\) so it is not shown.</td>
</tr>

<tr>
<td class="org-left">\(5x-3=0\)</td>
<td class="org-left">This is not quadratic as it is missing \(x^2\).</td>
</tr>
</tbody>
</table>

<p>
(source: <a href="https://www.mathsisfun.com/algebra/quadratic-equation.html">mathsisfun</a>)
</p>

<p>
To better understand why \(5x-3=0\) is not quadratic, one should keep in mind that the equation must fit the form \(ax^2+bx+c=0\). For the given equation to be written in that form, it would look like:
</p>

<p>
\[0x^2+5x-3=0\]
</p>

<p>
where:
</p>

<ul class="org-ul">
<li>\(a=0\)</li>
<li>\(b=-5\)</li>
<li>\(c=-3\)</li>
</ul>

<p>
Since \(a=0\), the equation does not meet the criteria for being quadratic. A quadratic equation requires that \(a\) (the coefficient of \(x^2\)) <b>must be non-zero</b>. Therefore, this equation is a <b>linear</b> equation, which can be written as:
</p>

<p>
\[ax+b=0\]
</p>
</div>
<div id="outline-container-org50e3b48" class="outline-5">
<h5 id="org50e3b48">Hidden quadratic equations</h5>
<div class="outline-text-5" id="text-org50e3b48">
<p>
Sometimes a quadratic equation does not look like \(5x^2+3x+3=0\), for example:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Initial form</th>
<th scope="col" class="org-left">Converting to standard form</th>
<th scope="col" class="org-left">In standard form</th>
<th scope="col" class="org-left">a, b and c</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(x^2=3x-1\)</td>
<td class="org-left">Move all terms to left hand side</td>
<td class="org-left">\(x^2-3x+1=0\)</td>
<td class="org-left">\(a=1\), \(b=3\) and \(c=1\)</td>
</tr>

<tr>
<td class="org-left">\(2(w^2-2w)=5\)</td>
<td class="org-left">Expand (undo the brackets) and move 5 to the left</td>
<td class="org-left">\(2w^2-4w-5=0\)</td>
<td class="org-left">\(a=2\), \(b=4\) and \(c=5\)</td>
</tr>

<tr>
<td class="org-left">\(z(z-1)=3\)</td>
<td class="org-left">Expand, and move 3 to left</td>
<td class="org-left">\(z^2-z-3=0\)</td>
<td class="org-left">\(a=1\), \(b=-1\) and \(c=3\)</td>
</tr>
</tbody>
</table>

<p>
(source: <a href="https://www.mathsisfun.com/algebra/quadratic-equation.html">mathsisfun</a>)
</p>
</div>
</div>
<div id="outline-container-org270c666" class="outline-5">
<h5 id="org270c666">Solving quadratic equations</h5>
<div class="outline-text-5" id="text-org270c666">
<p>
The <i>solutions</i> to a quadratic equation is where the value of \(x\) that make the equation equal to zero. These solution are also called <i>roots</i> or sometimes <i>zeroes</i>.
</p>

<p>
There are usually two solutions to a quadratic equation, as per this graph
</p>


<div id="org1a4168f" class="figure">
<p><img src="img/quadratic_graph.png" alt="quadratic_graph.png" width="250px" />
</p>
<p><span class="figure-number">Figure 2: </span>Quadratic graph</p>
</div>

<p>
and they can be solved using the following <i>quadratic formula</i>:
</p>

<p>
\[x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}\]
</p>

<p>
Here the \(\pm\) sign means that there two answers:
</p>

<p>
\[x = \frac{-b + \sqrt{b^2 - 4ac}}{2a}\]
</p>

<p>
and
</p>

<p>
\[x = \frac{-b - \sqrt{b^2 - 4ac}}{2a}\]
</p>

<p>
The term under the square root, \(b^2-4ac\) is called the <a id="org810f0ac"></a>/discriminant/, because it can <b>discriminate</b> between the possible types of solutions:
</p>

<ul class="org-ul">
<li>If \(b^2-4ac>0\), there are two distinct <i>real</i> solutions.</li>
<li>If \(b^2-4ac=0\), there is one real solution (a repeated root).</li>
<li>If \(b^2-4ac<0\), there are no real solutions, but two complex solutions.</li>
</ul>
</div>
</div>
<div id="outline-container-org293e665" class="outline-5">
<h5 id="org293e665">Using the quadratic formula</h5>
<div class="outline-text-5" id="text-org293e665">
<ul class="org-ul">
<li>Coefficients are: \[a=5 b=6 c=1\]</li>
<li>Quadratic formula: \[x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}\]</li>
<li>Put in a, b and c: \[x = \frac{-6 \pm \sqrt{6^2 - 451}}{25}\]</li>
<li>Solve: \[x = \frac{-6 \pm \sqrt{36 - 20}}{10}\] \[x = \frac{-6 \pm \sqrt{16}}{10}\] \[x = \frac{-6 \pm 4}{10}\] \[x = -0.2 \text{ or } -1\]</li>
</ul>

<p>
The solutions can be visualised on this graph
</p>


<div id="org578d54a" class="figure">
<p><img src="img/quadratic_graph_2.png" alt="quadratic_graph_2.png" width="350px" />
</p>
<p><span class="figure-number">Figure 3: </span>Quadratic graph real solutions</p>
</div>

<p>
We can check the answers:
</p>

<ul class="org-ul">
<li><p>
Check -0.2
</p>

<p>
\[5*(-0.2)^2+6*(-0.2)+1=5*(0.04)+6*(0.2)+1=0.2-1.2+1=0\]
</p></li>

<li><p>
Check -1
</p>

<p>
\[5*(-1)^2+6*(-1)+1=5*(1)+6*(-1)+1=5-6+1=0\]
</p></li>
</ul>

<p>
If the solution was a repeated root, the parabola on the graph would only touch the \(x\) axis at one point
</p>


<div id="orgda04035" class="figure">
<p><img src="img/quadratic_graph_3.png" alt="quadratic_graph_3.png" width="350px" />
</p>
<p><span class="figure-number">Figure 4: </span>Quadratic graph repeated root</p>
</div>

<p>
(source: <a href="https://www.mathsisfun.com/algebra/quadratic-equation.html">mathsisfun</a>)
</p>
</div>
<div id="outline-container-orga28b2c2" class="outline-6">
<h6 id="orga28b2c2">Complex solutions</h6>
<div class="outline-text-6" id="text-orga28b2c2">
<p>
When the <a href="#org810f0ac">discriminant</a> is negative we get a pair of <b>complex</b> solutions, meaning that the results will include <b>imaginary numbers</b>. For example, if we wanted to solve \[5x^2+2x+1=0\]
</p>

<p>
with coefficients \(a=5\), \(b=2\) and \(c=1\)
</p>

<p>
in this case the discriminant is negative \[b^2-4ac=2^2-4*5*1=-16\]
</p>

<p>
Using the quadratic formula \[x = \frac{-2 \pm \sqrt{-16}}{10}\] \[\sqrt{(-16)} = 4i\]
</p>

<p>
therefore \[x = \frac{-2 \pm 4i}{10} = -0.2 \pm 0.4i\]
</p>

<p>
Viewing on a graph, the curve does not cross the \(x\) axis, which is why we end up with a complex number
</p>


<div id="orge875628" class="figure">
<p><img src="img/quadratic_graph_4.png" alt="quadratic_graph_4.png" width="350px" />
</p>
<p><span class="figure-number">Figure 5: </span>Quadratic graph complex solution</p>
</div>

<p>
(source: <a href="https://www.mathsisfun.com/algebra/quadratic-equation.html">mathsisfun</a>)
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgdb2c2c8" class="outline-4">
<h4 id="orgdb2c2c8">Tensor broadcasting</h4>
<div class="outline-text-4" id="text-orgdb2c2c8">
<p>
According to <a href="https://numpy.org/doc/stable/user/basics.broadcasting.html">numpy</a>, tensor operations are usually done on pairs of tensors on an element-by-element basis. In the simplest case, the two tensors must have the exact same shape. For example:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">*</span> [<span style="color: #a9a1e1;">1.0</span> <span style="color: #a9a1e1;">2.0</span> <span style="color: #a9a1e1;">3.0</span>] [<span style="color: #a9a1e1;">2.0</span> <span style="color: #a9a1e1;">2.0</span> <span style="color: #a9a1e1;">2.0</span>])
</pre>
</div>

<p>
will return:
</p>

<div class="org-src-container">
<pre class="src src-racket">[<span style="color: #a9a1e1;">2.0</span> <span style="color: #a9a1e1;">4.0</span> <span style="color: #a9a1e1;">6.0</span>]
</pre>
</div>

<p>
Broadcasting rules can <b>relax</b> these constraints when the tensors' shapes meet certain constraints. The simplest broadcasting example occurs when a tensor and a scalar are combined in an operation:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">*</span> [<span style="color: #a9a1e1;">1.0</span> <span style="color: #a9a1e1;">2.0</span> <span style="color: #a9a1e1;">3.0</span>] <span style="color: #a9a1e1;">2.0</span>)
</pre>
</div>

<p>
returns:
</p>

<div class="org-src-container">
<pre class="src src-racket">[<span style="color: #a9a1e1;">2.0</span> <span style="color: #a9a1e1;">4.0</span> <span style="color: #a9a1e1;">6.0</span>]
</pre>
</div>

<p>
We can think of the scalar being <b>stretched</b> during the arithmetic operation, into a tensor the same shape as the first one. The new elements in the stretched tensor are just copies of the original scalar.
</p>


<div id="orga0f180f" class="figure">
<p><img src="img/broadcasting_1.png" alt="broadcasting_1.png" />
</p>
<p><span class="figure-number">Figure 6: </span>In the simplest example of broadcasting, the scalar <b>b</b> is stretched to become an array of same shape as <b>a</b> so the shapes are compatible for element-by-element multiplication (source: <a href="https://numpy.org/doc/stable/user/basics.broadcasting.html">numpy</a>).</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgc578a0d" class="outline-3">
<h3 id="orgc578a0d">Little learner</h3>
<div class="outline-text-3" id="text-orgc578a0d">
<p>
This section is a series of notes about machine learning and deep learning in the context of the book <a href="https://www.thelittlelearner.com/">"The Little Learner: A straight line to deep learning"</a> by <i>Daniel P. Friedman and Anurag Mintaka</i>.
</p>
</div>
<div id="outline-container-org0321f62" class="outline-4">
<h4 id="org0321f62">Basic machine learning</h4>
<div class="outline-text-4" id="text-org0321f62">
<p>
Here's a plane graph plot of a line against the \(x\) axis (<b>horizontal</b>) and the \(y\) axis (<b>vertical</b>).
</p>


<div id="orgf1f39d9" class="figure">
<p><img src="img/LL_line_1.png" alt="LL_line_1.png" width="250px" />
</p>
<p><span class="figure-number">Figure 7: </span>Plane graph plot</p>
</div>

<p>
\(x\) is related with \(y\) for every point \((x, y)\) on the line by an <i>equation</i>.<br />
Using arrows at both ends of the line means that it extends indefinitely in both directions. It follows that that there is a corresponding \(y\) for every \(x\).
</p>


<div id="orgf2e9c4e" class="figure">
<p><img src="img/LL_line_2.png" alt="LL_line_2.png" width="250px" />
</p>
<p><span class="figure-number">Figure 8: </span>Plane graph plot</p>
</div>

<p>
The point at which the \(x\) axis and the \(y\) axis meet is called the <i>origin</i>, the point (0, 0). The line in this particular graph passes through it.<br />
Because this line passes through the origin, \(y\) is a multiple of \(x\) by a constant factor \(w\) (weight). \(w\) is called the <i>slope</i> of the line and the aforementioned equation is \(y=wx\)
</p>


<div id="org9190220" class="figure">
<p><img src="img/LL_line_3.png" alt="LL_line_3.png" width="250px" />
</p>
<p><span class="figure-number">Figure 9: </span>Plane graph plot</p>
</div>

<p>
In case the line does not go through the origin, this is what it looks like
</p>


<div id="orgc687720" class="figure">
<p><img src="img/LL_line_4.png" alt="LL_line_4.png" width="250px" />
</p>
<p><span class="figure-number">Figure 10: </span>Plane graph plot</p>
</div>

<p>
Now the whole line is lifted by \(b\) (sometimes called the <i>bias</i>), and \(y\) for any \(x\) can be determined with the equation \(y=wx+b\).
</p>

<p>
Here's an initial attempt at a <a id="orgdc9ba80"></a> <i>line</i> function, <b>it is a function of one argument which results in a function of two arguments</b>:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #51afef;">define</span> <span style="color: #dcaeea;">line</span>
  (<span style="color: #51afef;">lambda</span> (x)
    (<span style="color: #51afef;">lambda</span> (w b)
      (<span style="color: #c678dd;">+</span> (<span style="color: #c678dd;">*</span> w x) b))))
</pre>
</div>

<p>
Although this definition of <i>line</i> might seem not to accept the arguments of \(w\), \(b\) and \(x\) be in the correct order, one ought to understand that the kind of problem we are dealing with is where the values of \(x\) are known, and \(w\) and \(b\) must be figured out from number of given values of \(x\) and \(y\).<br />
If we had \(w\) and \(b\) before \(x\), this would assume that they are known <b>prior</b> to the argument \(x\).
</p>

<p>
Now \(w\) and \(b\) are used to determine the \(y\) corresponding to a given \(x\), they are considered a special kind of argument. They are referred to as <i>parameters</i> of <i>line</i>, whereas \(x\) is the <i>argument</i> of <i>line</i>.
</p>

<p>
Here's an example of how to use <i>line</i>:
</p>

<div class="org-src-container">
<pre class="src src-racket">(line <span style="color: #a9a1e1;">8</span>)
</pre>
</div>

<p>
where (<i>line</i> 8) <b>is itself function</b> which remembers that \(x\) is 8, and is waiting to accept arguments (i.e. parameters) for its arguments \(w\) and \(b\), like this:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #51afef;">lambda</span> (w b)
  (<span style="color: #c678dd;">+</span> (<span style="color: #c678dd;">*</span> w <span style="color: #a9a1e1;">8</span>) b))
</pre>
</div>

<p>
So when (<i>line</i> 8) is invoked on \(w\) and \(b\), we can determine \(y\).<br />
If we were to invoke:
</p>

<div class="org-src-container">
<pre class="src src-racket">((line <span style="color: #a9a1e1;">8</span>) <span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">6</span>)
</pre>
</div>

<p>
those would be the steps:
</p>

<div class="org-src-container">
<pre class="src src-racket">((<span style="color: #51afef;">lambda</span> (w b)
   (<span style="color: #c678dd;">+</span> (<span style="color: #c678dd;">*</span> w <span style="color: #a9a1e1;">8</span>) b)) <span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">6</span>)

(<span style="color: #c678dd;">+</span> (<span style="color: #c678dd;">*</span> <span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">8</span>) <span style="color: #a9a1e1;">6</span>)

(<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">32</span> <span style="color: #a9a1e1;">6</span>)

<span style="color: #51afef;">=&gt;</span> <span style="color: #a9a1e1;">38</span>
</pre>
</div>

<p>
meaning that when \(x\) is 8, y is 38.
</p>

<p>
Functions that accept parameters <b>after</b> the arguments are known as <a href="#org39437b3">parameterized functions</a>, <i>line</i> is a parameterized function because it takes \(w\) and \(b\) after the argument \(x\).<br />
Such functions are used where the correct values for the parameters (here \(w\) and \(b\)) must be figured from given values of \(x\) and the corresponding values of \(y\).<br />
Here is an example, using the following <a id="orgac6458a"></a>dataset:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #51afef;">define</span> <span style="color: #dcaeea;">line-xs</span>
  [<span style="color: #a9a1e1;">2.0</span> <span style="color: #a9a1e1;">1.0</span> <span style="color: #a9a1e1;">4.0</span> <span style="color: #a9a1e1;">3.0</span>])

(<span style="color: #51afef;">define</span> <span style="color: #dcaeea;">line-ys</span>
  [<span style="color: #a9a1e1;">1.8</span> <span style="color: #a9a1e1;">1.2</span> <span style="color: #a9a1e1;">4.2</span> <span style="color: #a9a1e1;">3.3</span>])
</pre>
</div>

<p>
for each \(x\) coordinate in <i>line-xs</i>, there is a corresponding \(y\) coordinate in <i>line-ys</i>.<br />
That's how the dataset <i>(line-xs, line-ys)</i> looks as points on a graph:
</p>


<div id="org1467f04" class="figure">
<p><img src="img/LL_line_5.png" alt="LL_line_5.png" width="250px" />
</p>
<p><span class="figure-number">Figure 11: </span>Dataset plotted on plane graph</p>
</div>

<p>
Going back to our <i>line</i> function, remember that it has two parameters, \(w\) and \(b\). If we were to draw a line close the four points, like this:
</p>


<div id="orgf4496c4" class="figure">
<p><img src="img/LL_line_6.png" alt="LL_line_6.png" width="250px" />
</p>
<p><span class="figure-number">Figure 12: </span>Dataset with line plotted</p>
</div>

<p>
we would say that this function is a simple line with parameters \(w\) = 1.0 and \(b\) = 0.0, since the line passes trough the origin, \(b\) is 0.0.<br />
The \(x\) and \(y\) coordinates <b>on this line are always equal</b>. For instance, (0.0 0.0), (1.0 1.0), and so on. This is why \(w\) is 1.0.<br />
Therefore, if we are given a new \(x\) coordinate, it's possible to <i>predict</i> the corresponding \(y\) coordinate from this <i>line</i> function. For instance if \(x = 3.79\), then \[y=1.0*3.79+0=3.79\]
</p>

<p>
This would be referred to as the <i>predicted</i> \(y\) for a given \(x\). Finding the parameters of a function from a dataset is called <i>learning</i>.<br />
The parameters \(w\) and \(b\) are collectively known as the <i>parameter set, which is referenced to as &theta; (little-theta). &theta; in this case has two parameters, the first which is \(w\), is referred to as \(\theta_0\), and \(b\) is \(\theta_1\).<br />
Our /line</i> function can then be rewritten like this:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #51afef;">define</span> <span style="color: #dcaeea;">line</span>
  (<span style="color: #51afef;">lambda</span> (x)
    (<span style="color: #51afef;">lambda</span> (little-theta)
      (<span style="color: #c678dd;">+</span> (<span style="color: #c678dd;">*</span> (ref little-theta <span style="color: #a9a1e1;">0</span>) x) (ref little-theta <span style="color: #a9a1e1;">1</span>)))))
</pre>
</div>

<p>
So the dataset was examined on a graph, and from that it was possible to <b>estimate</b> a \(\theta\). This was used to <b>predict</b> a \(y\) coordinate for an \(x\) coordinate which might not be part of the initial dataset.
</p>

<p>
One final thing to note is that <i>line</i> is what's commonly called a <i>linear function</i> because it only uses addition and <i>scaling</i> to find its result. Scaling multiplies its argument by a fixed value or by a parameter. For example:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">*</span> <span style="color: #a9a1e1;">5.0</span> x)
</pre>
</div>

<p>
scales the value of \(x\) by 5.0. If we had a parameter \(\theta_0\), then
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">*</span> (ref theta <span style="color: #a9a1e1;">0</span>) x)
</pre>
</div>

<p>
scales the value of \(x\) by \(\theta_0\).
This explains why <i>line</i> is a linear function, it scales its argument \(x\) with \(\theta_0\) and <b>adds</b> \(\theta_1\) to it. This is in contrast to <a href="#orgad31dd3">non-linear</a> functions, such as quadratic functions which use squaring.
</p>
</div>
</div>
<div id="outline-container-orgc631602" class="outline-4">
<h4 id="orgc631602">Functions</h4>
<div class="outline-text-4" id="text-orgc631602">
</div>
<div id="outline-container-orgf252d17" class="outline-5">
<h5 id="orgf252d17">Expectant and objective functions</h5>
<div class="outline-text-5" id="text-orgf252d17">
<p>
A function invocation such as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(l2-loss line)
</pre>
</div>

<p>
which in a "same-as" chart transcribes as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #51afef;">lambda</span> (xs ys)
  (<span style="color: #51afef;">lambda</span> (theta)
    (<span style="color: #51afef;">let</span> ((<span style="color: #dcaeea;">pred-ys</span> ((line xs) theta)))
      (sum
       (<span style="color: #c678dd;">sqr</span>
        (<span style="color: #c678dd;">-</span> ys pred-ys)))))
</pre>
</div>

<p>
produces another function. This function which is produced when <b>l2-loss</b> is invoked with a <b>target</b> function (<b>line</b> in this case), is referred to as an <b>expectant</b> function. This is because it is <b>expecting</b> a data set as arguments.
</p>

<p>
When an expectant function receives a data set, for example:
</p>

<div class="org-src-container">
<pre class="src src-racket">((l2-loss line) line-xs line-ys)
</pre>
</div>

<p>
which in a "same-as" chart transcribes as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #51afef;">lambda</span> (theta)
  (<span style="color: #51afef;">let</span> ((<span style="color: #dcaeea;">pred-ys</span> ((line line-xs) theta)))
    (sum
     (<span style="color: #c678dd;">sqr</span>
      (<span style="color: #c678dd;">-</span> line-ys pred-ys)))))
</pre>
</div>

<p>
it produces a function which <b>awaits</b> a &theta;. The name of the produced function is known as an <a id="orge38f97b"></a>objective function. When provided with a \(\theta\), the objective function returns a scalar representing the <a href="#orgd426ad4">loss</a>, which is a measure of how far away we are from the well fitted \(\theta\).
</p>

<p>
The objective function would be called as such:
</p>

<div class="org-src-container">
<pre class="src src-racket">(((l2-loss line) line-xs line-ys) (<span style="color: #c678dd;">list</span> <span style="color: #a9a1e1;">0.0</span> <span style="color: #a9a1e1;">0.0</span>))
</pre>
</div>

<p>
<b>Key point</b>:
</p>

<p>
One ought to be mindful that
</p>

<div class="org-src-container">
<pre class="src src-racket">((l2-loss line) line-xs line-ys)
</pre>
</div>

<p>
is <b>not</b> a direct function call. It is itself a function which takes a \(\theta\) as input.
</p>
</div>
</div>
<div id="outline-container-org9481b3f" class="outline-5">
<h5 id="org9481b3f">Extended functions</h5>
<div class="outline-text-5" id="text-org9481b3f">
<ul class="org-ul">
<li><p>
In the context of this book, some functions were built to work on both scalars and tensors. They are referred as <b>extended</b> functions.<br />
For instance, the function:
</p>

<div class="org-src-container">
<pre class="src src-racket"><span style="color: #c678dd;">+</span>
</pre>
</div>

<p>
is one of them.<br />
Other functions that work with scalars can be extended similarly.
</p></li>

<li><p>
In its "regular" (i.e. non-extended) way, the function + works as such:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">1</span> <span style="color: #a9a1e1;">1</span>)
<span style="color: #a9a1e1;">5</span>
</pre>
</div>

<p>
However, because + is built using <b>extension</b> it can also work on tensors:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">+</span> [<span style="color: #a9a1e1;">2</span>] [<span style="color: #a9a1e1;">7</span>])
[<span style="color: #a9a1e1;">9</span>]
</pre>
</div>

<p>
here is a "same-as" chart to illustrate how this results in [9]:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">+</span> [<span style="color: #a9a1e1;">2</span>] [<span style="color: #a9a1e1;">7</span>])
[(<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">2</span> <span style="color: #a9a1e1;">7</span>)]
[<span style="color: #a9a1e1;">9</span>]
</pre>
</div>

<p>
When there is a function invocation like + on tensors, we look inside those tensors to determine the invocation's final value.
</p>

<p>
Here is another example:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">+</span> [<span style="color: #a9a1e1;">5</span> <span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">7</span>] [<span style="color: #a9a1e1;">2</span> <span style="color: #a9a1e1;">0</span> <span style="color: #a9a1e1;">1</span>])
</pre>
</div>

<p>
transcribes as:
</p>

<div class="org-src-container">
<pre class="src src-racket">[(<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">5</span> <span style="color: #a9a1e1;">2</span>) (<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">0</span>) (<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">7</span> <span style="color: #a9a1e1;">1</span>)]
</pre>
</div>

<p>
and results in:
</p>

<div class="org-src-container">
<pre class="src src-racket">[<span style="color: #a9a1e1;">7</span> <span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">8</span>]
</pre>
</div>

<p>
It's as if + <b>descends</b> into its tensor<sup>1</sup> arguments to results in another tensor<sup>1</sup>. The last step results in the tensor<sup>1</sup> of the values of the three sums.
</p>

<p>
We can also add 2 tensors<sup>2</sup> of the same shape, for instance:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">+</span> [[<span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">7</span>] [<span style="color: #a9a1e1;">2</span> <span style="color: #a9a1e1;">0</span> <span style="color: #a9a1e1;">1</span>]]
   [[<span style="color: #a9a1e1;">1</span> <span style="color: #a9a1e1;">2</span> <span style="color: #a9a1e1;">2</span>] [<span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">3</span> <span style="color: #a9a1e1;">1</span>]])
</pre>
</div>

<p>
transcribes as:
</p>

<div class="org-src-container">
<pre class="src src-racket">[(<span style="color: #c678dd;">+</span> [<span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">7</span>] [<span style="color: #a9a1e1;">1</span> <span style="color: #a9a1e1;">2</span> <span style="color: #a9a1e1;">2</span>])
 (<span style="color: #c678dd;">+</span> [<span style="color: #a9a1e1;">2</span> <span style="color: #a9a1e1;">0</span> <span style="color: #a9a1e1;">1</span>] [<span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">3</span> <span style="color: #a9a1e1;">1</span>])]

[[(<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">1</span>) (<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">2</span>) (<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">7</span> <span style="color: #a9a1e1;">2</span>)]
 [(<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">2</span> <span style="color: #a9a1e1;">6</span>) (<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">0</span> <span style="color: #a9a1e1;">3</span>) (<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">1</span> <span style="color: #a9a1e1;">1</span>)]]
</pre>
</div>

<p>
and results in:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[<span style="color: #a9a1e1;">5</span> <span style="color: #a9a1e1;">8</span> <span style="color: #a9a1e1;">9</span>] [<span style="color: #a9a1e1;">8</span> <span style="color: #a9a1e1;">3</span> <span style="color: #a9a1e1;">2</span>]]
</pre>
</div>

<p>
The authors then go on to explain that tensors <b>must</b> be of the same shape before they can be added together, and that getting functions such as + to work on tensors of <b>arbitrary</b> ranks is known as <b>pointwise extension</b>. This however seems to be more commonly known as <a href="#orgcbd8c67">elementwise operations</a>.
</p></li>

<li><p>
The point by the authors that tensors <b>must</b> have the same shape before computation can be performed on them is, later in the text, contradicted and unfortunately not very well explained. Nevertheless we are shown that the following operation can actually be performed on tensors:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">4</span> [<span style="color: #a9a1e1;">3</span> <span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">5</span>])
</pre>
</div>

<p>
by doing this:
</p>

<div class="org-src-container">
<pre class="src src-racket">[(<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">3</span>) (<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">6</span>) (<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">5</span>)]
</pre>
</div>

<p>
which results in:
</p>

<div class="org-src-container">
<pre class="src src-racket">[<span style="color: #a9a1e1;">7</span> <span style="color: #a9a1e1;">10</span> <span style="color: #a9a1e1;">9</span>]
</pre>
</div>

<p>
Another example given is:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">+</span> [<span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">9</span> <span style="color: #a9a1e1;">1</span>] [[<span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">3</span> <span style="color: #a9a1e1;">8</span>] [<span style="color: #a9a1e1;">7</span> <span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">7</span>]])
</pre>
</div>

<p>
and in this case, we can look inside the tensor<sup>2</sup> argument and add the tensor<sup>1</sup> argument just as we did in the previous addition:
</p>

<div class="org-src-container">
<pre class="src src-racket">[(<span style="color: #c678dd;">+</span> [<span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">9</span> <span style="color: #a9a1e1;">1</span>] [<span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">3</span> <span style="color: #a9a1e1;">8</span>])
 (<span style="color: #c678dd;">+</span> [<span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">9</span> <span style="color: #a9a1e1;">1</span>] [<span style="color: #a9a1e1;">7</span> <span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">7</span>])]

[[(<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">4</span>) (<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">9</span> <span style="color: #a9a1e1;">3</span>) (<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">1</span> <span style="color: #a9a1e1;">8</span>)]
 [(<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">7</span>) (<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">9</span> <span style="color: #a9a1e1;">4</span>) (<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">1</span> <span style="color: #a9a1e1;">7</span>)]]
</pre>
</div>

<p>
which results in:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[<span style="color: #a9a1e1;">10</span> <span style="color: #a9a1e1;">12</span> <span style="color: #a9a1e1;">9</span>] [<span style="color: #a9a1e1;">13</span> <span style="color: #a9a1e1;">13</span> <span style="color: #a9a1e1;">8</span>]]
</pre>
</div>

<p>
This is in fact called <a href="#orgdb2c2c8">tensor broadcasting</a>.
</p></li>

<li><p>
The reader is subsequently introduced to the extended version of:
</p>

<div class="org-src-container">
<pre class="src src-racket"><span style="color: #c678dd;">*</span>
</pre>
</div>

<p>
with the following operation:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">*</span> [[<span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">5</span>] [<span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">9</span> <span style="color: #a9a1e1;">7</span>]] <span style="color: #a9a1e1;">3</span>)
</pre>
</div>

<p>
and these are the steps:
</p>

<div class="org-src-container">
<pre class="src src-racket">[(<span style="color: #c678dd;">*</span> [<span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">5</span>] <span style="color: #a9a1e1;">3</span>) (<span style="color: #c678dd;">*</span> [<span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">9</span> <span style="color: #a9a1e1;">7</span>] <span style="color: #a9a1e1;">3</span>)]

[[(<span style="color: #c678dd;">*</span> <span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">3</span>) (<span style="color: #c678dd;">*</span> <span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">3</span>) (<span style="color: #c678dd;">*</span> <span style="color: #a9a1e1;">5</span> <span style="color: #a9a1e1;">3</span>)]
 [(<span style="color: #c678dd;">*</span> <span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">3</span>) (<span style="color: #c678dd;">*</span> <span style="color: #a9a1e1;">9</span> <span style="color: #a9a1e1;">3</span>) (<span style="color: #c678dd;">*</span> <span style="color: #a9a1e1;">7</span> <span style="color: #a9a1e1;">3</span>)]]
</pre>
</div>

<p>
which returns:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[<span style="color: #a9a1e1;">12</span> <span style="color: #a9a1e1;">18</span> <span style="color: #a9a1e1;">15</span>] [<span style="color: #a9a1e1;">18</span> <span style="color: #a9a1e1;">27</span> <span style="color: #a9a1e1;">14</span>]]
</pre>
</div>

<p>
We are told that this is the <b>Hadamard multiplication</b>.<br />
Upon further research this does not appear to fit the definition of the Hadamard multiplication, or rather <a href="#org6f349a0">Hadamard product</a>, but a simple tensor to scalar multiplication which involves broadcasting.
</p></li>

<li><p>
Other extended functions such as:
</p>

<div class="org-src-container">
<pre class="src src-racket"><span style="color: #c678dd;">sqrt</span>
</pre>
</div>

<p>
descend into tensors. In the case of a tensor<sup>1</sup>, this is how sqrt works:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">sqrt</span> [<span style="color: #a9a1e1;">9</span> <span style="color: #a9a1e1;">16</span> <span style="color: #a9a1e1;">25</span>])

[(<span style="color: #c678dd;">sqrt</span> <span style="color: #a9a1e1;">9</span>) (<span style="color: #c678dd;">sqrt</span> <span style="color: #a9a1e1;">16</span>) (<span style="color: #c678dd;">sqrt</span> <span style="color: #a9a1e1;">25</span>)]
</pre>
</div>

<p>
which returns:
</p>

<div class="org-src-container">
<pre class="src src-racket">[<span style="color: #a9a1e1;">3</span> <span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">5</span>]
</pre>
</div>

<p>
In the case of a tensor<sup>2</sup>, it works this way:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">sqrt</span> [[<span style="color: #a9a1e1;">49</span> <span style="color: #a9a1e1;">81</span> <span style="color: #a9a1e1;">16</span>] [<span style="color: #a9a1e1;">64</span> <span style="color: #a9a1e1;">25</span> <span style="color: #a9a1e1;">36</span>]])

[(<span style="color: #c678dd;">sqrt</span> [[<span style="color: #a9a1e1;">49</span> <span style="color: #a9a1e1;">81</span> <span style="color: #a9a1e1;">16</span>]]) (<span style="color: #c678dd;">sqrt</span> [[<span style="color: #a9a1e1;">64</span> <span style="color: #a9a1e1;">25</span> <span style="color: #a9a1e1;">36</span>]])]

[[(<span style="color: #c678dd;">sqrt</span> <span style="color: #a9a1e1;">49</span>) (<span style="color: #c678dd;">sqrt</span> <span style="color: #a9a1e1;">81</span>) (<span style="color: #c678dd;">sqrt</span> <span style="color: #a9a1e1;">16</span>)]
 [(<span style="color: #c678dd;">sqrt</span> <span style="color: #a9a1e1;">64</span>) (<span style="color: #c678dd;">sqrt</span> <span style="color: #a9a1e1;">25</span>) (<span style="color: #c678dd;">sqrt</span> <span style="color: #a9a1e1;">36</span>)]]
</pre>
</div>

<p>
and this results in:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[<span style="color: #a9a1e1;">7</span> <span style="color: #a9a1e1;">9</span> <span style="color: #a9a1e1;">4</span>] [<span style="color: #a9a1e1;">8</span> <span style="color: #a9a1e1;">5</span> <span style="color: #a9a1e1;">6</span>]]
</pre>
</div>

<p>
In other words, the function descends inside each tensor<sup>1</sup> until it finds a tensor<sup>0</sup> at which point it gets their square root.<br />
However, not all extended functions descend until they find scalars.  The function
</p>

<div class="org-src-container">
<pre class="src src-racket">sum
</pre>
</div>

<p>
which extends the function
</p>

<div class="org-src-container">
<pre class="src src-racket">sum-1
</pre>
</div>

<p>
descends into its argument until it finds a tensor<sup>1</sup> instead of a tensor<sup>0</sup>.<br />
This is how sum-1 is defined:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #51afef;">define</span> <span style="color: #dcaeea;">sum1</span>
  (<span style="color: #51afef;">lambda</span> (t)
    (summed t (<span style="color: #c678dd;">sub1</span> ) <span style="color: #a9a1e1;">0.0</span>)))

(<span style="color: #51afef;">define</span> <span style="color: #dcaeea;">summed</span>
  (<span style="color: #51afef;">lambda</span> (t i a)
    (<span style="color: #51afef;">cond</span>
      ((<span style="color: #c678dd;">zero?</span> i) (<span style="color: #c678dd;">+</span> (tref t <span style="color: #a9a1e1;">0</span>) a))
      (<span style="color: #51afef;">else</span>
       (summed t (<span style="color: #c678dd;">sub1</span> i) (<span style="color: #c678dd;">+</span> (tref t i) a))))))
</pre>
</div>

<p>
And this is how it behaves:
</p>

<div class="org-src-container">
<pre class="src src-racket">(sum-1 [<span style="color: #a9a1e1;">10.0</span> <span style="color: #a9a1e1;">12.0</span> <span style="color: #a9a1e1;">14.0</span>])
</pre>
</div>

<p>
which returns:
</p>

<div class="org-src-container">
<pre class="src src-racket"><span style="color: #a9a1e1;">32</span>
</pre>
</div>

<p>
Here's <i>sum</i> working on a tensor<sup>3</sup>:
</p>

<div class="org-src-container">
<pre class="src src-racket">(sum [[[<span style="color: #a9a1e1;">1</span> <span style="color: #a9a1e1;">2</span>] [<span style="color: #a9a1e1;">3</span> <span style="color: #a9a1e1;">4</span>]] [[<span style="color: #a9a1e1;">5</span> <span style="color: #a9a1e1;">6</span>] [<span style="color: #a9a1e1;">7</span> <span style="color: #a9a1e1;">8</span>]]])

[(sum [[<span style="color: #a9a1e1;">1</span> <span style="color: #a9a1e1;">2</span>] [<span style="color: #a9a1e1;">3</span> <span style="color: #a9a1e1;">4</span>]])
 (sum [[<span style="color: #a9a1e1;">5</span> <span style="color: #a9a1e1;">6</span>] [<span style="color: #a9a1e1;">7</span> <span style="color: #a9a1e1;">8</span>]])]

[[(sum-1 [<span style="color: #a9a1e1;">1</span> <span style="color: #a9a1e1;">2</span>]) (sum-1 [<span style="color: #a9a1e1;">3</span> <span style="color: #a9a1e1;">4</span>])]
 [(sum-1 [<span style="color: #a9a1e1;">5</span> <span style="color: #a9a1e1;">6</span>]) (sum-1 [<span style="color: #a9a1e1;">7</span> <span style="color: #a9a1e1;">8</span>])]]
</pre>
</div>

<p>
which results in:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[<span style="color: #a9a1e1;">3</span> <span style="color: #a9a1e1;">7</span>] [<span style="color: #a9a1e1;">11</span> <span style="color: #a9a1e1;">15</span>]]
</pre>
</div>

<p>
What can be noted about the rank of the resulting tensor is that it is <b>one rank less than the rank of the input</b>. In this case the input tensor was rank 3 and the output tensor is rank 2.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-orgd426ad4" class="outline-5">
<h5 id="orgd426ad4">Loss function</h5>
<div class="outline-text-5" id="text-orgd426ad4">
<p>
The loss function, also called <b>cost</b> or <b>error</b> function (source: <a href="https://en.wikipedia.org/wiki/Loss_function">wikipedia</a>), is a mathematical process that quantifies the error margin between a model's prediction and the actual target value. It acts as a guide for the learning process within a machine learning algorithm (source: <a href="https://www.datacamp.com/tutorial/loss-function-in-machine-learning">datacamp</a>), to tell us how far we are from a well-fitted \(\theta\).
</p>

<p>
A scalar is required in order to gauge how far we are from the well-fitted \(\theta\), which is known as the <b>loss</b>. The loss should be as close to 0.0 as possible.
</p>

<p>
This loss is determined every time \(\theta\) is revised, and since the loss shows how far we are from the well-fitted \(\theta\), it is used as guide for revising \(\theta\).
</p>

<p>
The most straightforward way to determine how far we are is by figuring out the difference between the given <i>ys</i> and the predicted <i>ys</i>, by using a function as such:
</p>

<div class="org-src-container">
<pre class="src src-racket">(sum
 (<span style="color: #c678dd;">-</span> line-ys ((line line-xs) (theta-0 theta-1))))
</pre>
</div>

<p>
This however, would not be good enough as if we had a dataset and a \(\theta\) that gave us a tensor difference of:
</p>

<div class="org-src-container">
<pre class="src src-racket">[<span style="color: #a9a1e1;">4.0</span> <span style="color: #a9a1e1;">-3.0</span> <span style="color: #a9a1e1;">0.0</span> <span style="color: #a9a1e1;">-4.0</span> <span style="color: #a9a1e1;">3.0</span>]
</pre>
</div>

<p>
the result of:
</p>

<div class="org-src-container">
<pre class="src src-racket">(sum [<span style="color: #a9a1e1;">4.0</span> <span style="color: #a9a1e1;">-3.0</span> <span style="color: #a9a1e1;">0.0</span> <span style="color: #a9a1e1;">-4.0</span> <span style="color: #a9a1e1;">3.0</span>])
</pre>
</div>

<p>
would return an ideal loss of 0.0, which is incorrect. As can be observed, although the individual difference are significant in most cases, the sum of 0.0 suggests that the \(\theta\) is a perfect fit.
</p>

<p>
This problem arises from <b>having negative values</b> in the argument to the function <i>sum</i>. The way this can be remedied is by <b>squaring</b> each element in the tensor difference, which turns each negative element to positive (and the positive elements remain positive), calling the function like this:
</p>

<div class="org-src-container">
<pre class="src src-racket">(sum
 (<span style="color: #c678dd;">sqr</span>
  (<span style="color: #c678dd;">-</span> line-ys ((line line-xs) (theta-0 theta-1)))))
</pre>
</div>

<p>
Now the sum of the squares is positive if at least one of the elements in the tensor difference is non-zero.<br />
This function is called <b>l2-loss</b> and should <b>not</b> be confused with <i>mean squared error</i> (or <i>MSE</i>). <i>MSE</i> would divide the sum of the squares by the number of elements in the tensor.
</p>

<p>
One thing to note about the l2-loss function is that it can in some case not perform well due to the presence of <a href="#org935e858">outliers</a>. This is because the squaring of an element from a tensor difference which belongs to an outlier can contribute disproportionately to the loss.
</p>
</div>
</div>
<div id="outline-container-org39437b3" class="outline-5">
<h5 id="org39437b3">Parameterized functions</h5>
<div class="outline-text-5" id="text-org39437b3">
<p>
Functions such as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #51afef;">define</span> <span style="color: #dcaeea;">line</span>
  (<span style="color: #51afef;">lambda</span> (x)
    (<span style="color: #51afef;">lambda</span> (theta)
      (<span style="color: #c678dd;">+</span> (<span style="color: #c678dd;">*</span> (ref theta <span style="color: #a9a1e1;">0</span>) x) (ref theta <span style="color: #a9a1e1;">1</span>)))))
</pre>
</div>

<p>
are known as <i>parameterized</i> functions.
</p>

<p>
Parameterized functions are used where we must figure out the right values for the parameters (here, \(\theta_0\) and \(\theta_1\)) from given values of \(x\) and the corresponding values of \(y\).
</p>

<p>
In this case when line is invoked, for example, with the <i>argument</i> 8 as such:
</p>

<div class="org-src-container">
<pre class="src src-racket">(line <span style="color: #a9a1e1;">8</span>)
</pre>
</div>

<p>
we can say that it is waiting to accept arguments for its parameters \(\theta_0\) and \(\theta_1\).
</p>

<p>
When (line 8) is invoked on \(\theta_0\) and \(\theta_1\) as such:
</p>

<div class="org-src-container">
<pre class="src src-racket">((line <span style="color: #a9a1e1;">8</span>) <span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">6</span>)
</pre>
</div>

<p>
we can then determine \(y\).
</p>
</div>
</div>
</div>
<div id="outline-container-orgd131526" class="outline-4">
<h4 id="orgd131526">Gradients</h4>
<div class="outline-text-4" id="text-orgd131526">
</div>
<div id="outline-container-orge69378d" class="outline-5">
<h5 id="orge69378d">Nomenclature</h5>
<div class="outline-text-5" id="text-orge69378d">
<ul class="org-ul">
<li><p>
<span class="underline">Gradient short definition</span>
</p>

<p>
A <i>gradient</i> is a general way of understanding the <a href="#org2f961e5">rate of change</a> of a parameterized function with respect to <b>all</b> its parameters.
</p></li>

<li><p>
<span class="underline">Gradient fancy name</span>
</p>

<p>
The gradient is a fancy word for derivative, or the rate of change of a function.
</p>

<p>
The term gradient is typically used for functions with several inputs and a single output (a scalar field). Yes, you can say a line has a gradient (its slope), but using "gradient" for single-variable functions is unnecessarily confusing (source: <a href="https://betterexplained.com/articles/vector-calculus-understanding-the-gradient/">betterexplained</a>).
</p></li>

<li><p>
<span class="underline"><a id="org0048001"></a>&nabla; function short explanation</span>
</p>

<p>
To find the gradient of a function at given values of its arguments, we need to use the function \(\nabla\) (i.e. <i>gradient-of</i>). The first argument to \(\nabla\) is a function \(f\) (an <a href="#orge38f97b">objective function</a>) that computes the <a href="#orgd426ad4">loss</a> (a scalar value) using a dataset and the list of parameters \(\theta\) as its input. The second argument to \(\nabla\) is the initial values of \(\theta\), the parameters for which we want to compute the gradients of the function \(f\).<br />
In other words &nabla; computes the gradient of <b>another function</b> with respect to its parameters.
</p>

<p>
The way to call \(\nabla\) is like this:
</p>

<div class="org-src-container">
<pre class="src src-racket">(gradient-of ((l2-loss line) line-xs line-ys) (<span style="color: #c678dd;">list</span> <span style="color: #a9a1e1;">0.0</span> <span style="color: #a9a1e1;">0.0</span>))
</pre>
</div>

<p>
and this is a detailed explanation of how it works:
</p>

<p>
<b>Key points</b>:
</p>

<ol class="org-ol">
<li><p>
The (<a href="#orge38f97b">objective</a>) function \(f\) generated by
</p>

<div class="org-src-container">
<pre class="src src-racket">((l2-loss line) line-xs line-ys)
</pre>
</div>

<p>
is designed to take \(\theta\) (<b>a list of tensors</b>) as its input.
</p></li>

<li>The \(\nabla\) function itself manages the passing of \(\theta\) to \(f\) internally.</li>

<li>Inside \(\nabla\):

<ul class="org-ul">
<li>\(f\) is invoked with <a href="#org5490c99">dual numbers</a> version of \(\theta\).</li>

<li>This invocation effectively passes the \(\theta\) values to \(f\).</li>
</ul></li>
</ol>

<p>
The result of the \(\nabla\) function is a list of gradients of the <a href="#orge38f97b">objective function</a> with respect to <b>each</b> parameter in \(\theta\), and is referred to as the <i>gradient list</i>.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org157580b" class="outline-5">
<h5 id="org157580b">Gradient descent</h5>
<div class="outline-text-5" id="text-org157580b">
<p>
Here is a graph of what is referred to (in this particular book) as a <b>loss curve</b>:
</p>


<div id="org7025b11" class="figure">
<p><img src="img/LL_loss_curve.png" alt="LL_loss_curve.png" />
</p>
<p><span class="figure-number">Figure 13: </span>Loss curve</p>
</div>

<p>
The <i>y</i>-axis represents the <a href="#orgd426ad4">loss</a> and the <i>x</i>-axis represents \(\theta_0\), which is also referred to as <b>weight</b>. So for any possible value of the weight, this graph shows the corresponding value of the loss <b>as an orange dot</b>.<br />
To draw this graph five weights (i.e. <b>five different values for</b> \(\theta_0\)) were chosen, -1.0, 0.0, 1.0, 2.0, and 3.0.
</p>

<p>
For each weight, we determine its corresponding loss while keeping \(\theta_1\) (i.e. the <b>bias</b>) at 0.0, using the following dataset:
</p>

<div class="org-src-container">
<pre class="src src-racket">[<span style="color: #a9a1e1;">2.0</span> <span style="color: #a9a1e1;">1.0</span> <span style="color: #a9a1e1;">4.0</span> <span style="color: #a9a1e1;">3.0</span>]
</pre>
</div>

<p>
for the \(x\) values (<i>line-xs</i>), and
</p>

<div class="org-src-container">
<pre class="src src-racket">[<span style="color: #a9a1e1;">1.8</span> <span style="color: #a9a1e1;">1.2</span> <span style="color: #a9a1e1;">4.2</span> <span style="color: #a9a1e1;">3.3</span>]
</pre>
</div>

<p>
for the \(y\) values (<i>line-ys</i>).<br />
Subsequently, this <a href="#orgf252d17">objective functions</a> (<i>obj</i>) is used:
</p>

<div class="org-src-container">
<pre class="src src-racket">((l2-loss line) line-xs line-ys)
</pre>
</div>

<p>
by providing it with a \(\theta\) which is constructed out of each of those five weights for \(\theta_0\), and 0.0 for \(\theta_1\) (the bias). In other words, the <i>obj</i> expression takes parameters as its arguments and returns a scalar which represents the loss.<br />
Those are the corresponding losses for each weight:
</p>

<div class="org-src-container">
<pre class="src src-racket">(obj (<span style="color: #c678dd;">list</span> <span style="color: #a9a1e1;">-1.0</span> <span style="color: #a9a1e1;">0.0</span>))

<span style="color: #51afef;">=&gt;</span> <span style="color: #a9a1e1;">126.21</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-racket">(obj (<span style="color: #c678dd;">list</span> <span style="color: #a9a1e1;">0.0</span> <span style="color: #a9a1e1;">0.0</span>))

<span style="color: #51afef;">=&gt;</span> <span style="color: #a9a1e1;">33.21</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-racket">(obj (<span style="color: #c678dd;">list</span> <span style="color: #a9a1e1;">1.0</span> <span style="color: #a9a1e1;">0.0</span>))

<span style="color: #51afef;">=&gt;</span> <span style="color: #a9a1e1;">0.21</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-racket">(obj (<span style="color: #c678dd;">list</span> <span style="color: #a9a1e1;">2.0</span> <span style="color: #a9a1e1;">0.0</span>))

<span style="color: #51afef;">=&gt;</span> <span style="color: #a9a1e1;">27.21</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-racket">(obj (<span style="color: #c678dd;">list</span> <span style="color: #a9a1e1;">3.0</span> <span style="color: #a9a1e1;">0.0</span>))

<span style="color: #51afef;">=&gt;</span> <span style="color: #a9a1e1;">114.21</span>
</pre>
</div>

<p>
This type of graph is used to show <i>quantities</i> that are <b>not</b> part of the dataset. In this context, <i>quantities</i> are values or metrics that are derived from the model and its training process (i.e. <b>the relationship between the loss and</b> \(\theta\)) rather than being directly observed data points (i.e. <b>these are not part of the raw input data</b>).<br />
In other words, the graph illustrates internal aspects of the model (like the parameter values and loss) rather than showing the raw input data directly. This distinction is important because it shifts the focus from the data itself to the model's behavior and performance during training.
</p>

<p>
We can see on the graph that an estimate weight value of 0.0 for \(\theta_0\) (whilst keeping \(\theta_1\) at 0.0) corresponds to a loss of 33.21. The point (0.0, 33.21) is circled in this graph:
</p>


<div id="orgd75b478" class="figure">
<p><img src="img/LL_loss_curve_2.png" alt="LL_loss_curve_2.png" />
</p>
<p><span class="figure-number">Figure 14: </span>Loss curve</p>
</div>

<p>
One can also observe that the loss is lowest at the <b>bottom</b> of the curve. Because of this, what we need to do is to <i>roll</i> down the curve to get to its bottom. The idea is to roll down as fast as possible without <a href="#org0a803f9">overshooting</a>, we use the <a href="#org547febf">rate of change</a> for this.
</p>

<p>
Here's how the rate of change is represented on the loss curve:
</p>


<div id="orgeea02f7" class="figure">
<p><img src="img/LL_loss_curve_3.png" alt="LL_loss_curve_3.png" />
</p>
<p><span class="figure-number">Figure 15: </span>Loss curve with tangent</p>
</div>

<p>
The turquoise line is known as a <i>tangent</i> and touches the curve at exactly <b>one</b> point, which in this case is (0.0 33.21). The rate of change determined is the <i>slope</i> of the tangent, it is called the <a href="#orge69378d">gradient</a>.
</p>

<p>
Now if we were to <a href="#org1e6608a">revise</a> our \(\theta_0\) by, say 0.6623, this is how the loss curve would look like:
</p>


<div id="org36e2771" class="figure">
<p><img src="img/LL_loss_curve_4.png" alt="LL_loss_curve_4.png" />
</p>
<p><span class="figure-number">Figure 16: </span>Loss curve with tangents</p>
</div>

<p>
What's interesting about this graph is that the second (revised) tangent is less steep compared to the tangent for the initial \(\theta_0\) estimate (of 0.0). Therefore, the gradient of the second is lower than the first.
</p>

<p>
In order to find the gradient we need to use the function <a href="#org0048001">&nabla;</a>.
</p>
</div>
</div>
<div id="outline-container-orgf4a1ecc" class="outline-5">
<h5 id="orgf4a1ecc">Optimizers</h5>
<div class="outline-text-5" id="text-orgf4a1ecc">
</div>
<div id="outline-container-orgef1c387" class="outline-6">
<h6 id="orgef1c387">Stochastic gradient descent</h6>
<div class="outline-text-6" id="text-orgef1c387">
<p>
The <a href="#orgac6458a">dataset</a> which we looked at previously contains very few points. Real datasets by comparison can have as much as <b>billion</b> of points. Our <a href="#orgd426ad4">loss function</a> uses the <b>entire</b> dataset when it is invoked, that's because it is finding the difference between the \(ys\) in the dataset and the corresponding predicted \(ys\).<br />
There is a better way which does not require traversing the entire dataset thousands of time, by means of <i>sampling</i>.
</p>

<p>
Using a small <b>random</b> sample of a few points from a large dataset will produce a good enough approximation of loss, which can be used to revise &theta;. This sample is referred to as a <i>batch</i>, and at each revision (i.e. <b>epoch</b>), only small fraction of the dataset is examined.
This is repeated over many revisions, with new samples for each revision, to get as close to the ideal loss as possible.
</p>

<p>
This kind of gradient descent where the <a href="#orgf252d17">objective functions</a> uses sampling is known as <i>stochastic gradient descent</i>. <i>Stochastic</i> is another way of saying that random numbers are used to determine the results. Using samples as part of the objective function is what makes it stochastic.
</p>
</div>
</div>
<div id="outline-container-orgad48faf" class="outline-6">
<h6 id="orgad48faf">Momentum gradient descent</h6>
<div class="outline-text-6" id="text-orgad48faf">
<p>
<i>Momentum gradient descent</i> is a method which reaches a well-fitted \(\theta\) with fewer revisions.<br />
Going back to this loss graph
</p>


<div id="orgacef3ae" class="figure">
<p><img src="img/LL_loss_curve_4.png" alt="LL_loss_curve_4.png" />
</p>
<p><span class="figure-number">Figure 17: </span>Loss curve with tangents</p>
</div>

<p>
we know that each tangent gets less and less steep as it approaches the bottom of the curve. In other words the slope of tangent (i.e. the gradient), gets smaller. In fact, as the bottom of the curve is approached, the gradient gets closer and closer to 0.0. Furthermore, at the very bottom of the curve, the gradient is exactly 0.0.<br />
What happens when a very small gradient is multiplied by a very small <a href="#org1848946">learning rate</a>, as in update functions, is that <b>we get something even smaller</b>. So at each <a href="#org1e6608a">revision</a> closer to the bottom, <b>the amount of change to each parameter gets smaller and smaller</b>.
</p>

<p>
Now, the change made to a given parameter at each revision is known as the <a id="orgd370e2c"></a> <i>velocity</i> of descent. Therefore we can say that the velocity of descent <b>slows down</b> as it approaches the bottom of the curve, <b>but</b>, it can be sped up.<br />
Here is an example function which updates \(\theta\):
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #51afef;">define</span> <span style="color: #dcaeea;">naked-u</span>
  (<span style="color: #51afef;">lambda</span> (P g)
    (&#8722; P (<span style="color: #c678dd;">*</span> alpha g))))
</pre>
</div>

<p>
in this expression since we subtract (* alpha g), the change to \(P\), (i.e., the velocity) is:
</p>

<div class="org-src-container">
<pre class="src src-racket">(&#8722; (<span style="color: #c678dd;">*</span> alpha g))
</pre>
</div>

<p>
The problem of the velocity of descent slowing down can be remedied by <b>boosting</b> our velocity. This is achieved by adding some fraction \(\mu\) of the velocity \(v\), of the previous revision, <b>to the change we expect to make in the current revision</b>.
</p>

<p>
Therefore our velocity which was:
</p>

<div class="org-src-container">
<pre class="src src-racket">(&#8722; (<span style="color: #c678dd;">*</span> alpha g))
</pre>
</div>

<p>
becomes:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">+</span> (<span style="color: #c678dd;">*</span> <span style="color: #c678dd;">mu</span> v) (<span style="color: #c678dd;">-</span> (<span style="color: #c678dd;">*</span> alpha g)))
</pre>
</div>

<p>
which is better written as:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">-</span> (<span style="color: #c678dd;">*</span> <span style="color: #c678dd;">mu</span> v) (<span style="color: #c678dd;">*</span> alpha g))
</pre>
</div>

<p>
where \(\mu\) is a hyperparameter between 0.0 and 1.0 (usually 0.9) which represents the decimal fraction of the previous velocity we want to retain for the next velocity. \(v\) represents the velocity of the most recent revision and it is an accompaniment of its corresponding parameter, which should have the same shape as its parameter.<br />
Here's the <i>inflate</i> function for the velocity algorithm:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #51afef;">define</span> <span style="color: #dcaeea;">velocity-i</span>
  (<span style="color: #51afef;">lambda</span> (p)
    (<span style="color: #c678dd;">list</span> p (zeroes p))))
</pre>
</div>

<p>
it adds an initial accompaniment to the parameter \(p\).<br />
The function <i>zeroes</i> produces a tensor with the same shape as its argument, made up entirely of 0.0s. Here's a chart for it:
</p>

<div class="org-src-container">
<pre class="src src-racket">(zeroes (tensor (tensor <span style="color: #a9a1e1;">2.1</span> <span style="color: #a9a1e1;">9.3</span> <span style="color: #a9a1e1;">1.5</span>) (tensor <span style="color: #a9a1e1;">7.2</span> <span style="color: #a9a1e1;">3.3</span> <span style="color: #a9a1e1;">6.6</span>)))

<span style="color: #51afef;">=&gt;</span> (tensor (tensor <span style="color: #a9a1e1;">0.0</span> <span style="color: #a9a1e1;">0.0</span> <span style="color: #a9a1e1;">0.0</span>) (tensor <span style="color: #a9a1e1;">0.0</span> <span style="color: #a9a1e1;">0.0</span> <span style="color: #a9a1e1;">0.0</span>))
</pre>
</div>

<p>
The reason for initially setting the velocity to be a zeroed tensor is because at the first revision, there hasn't been any change to any of the parameters.<br />
Here's the corresponding <i>deflate</i> function:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #51afef;">define</span> <span style="color: #dcaeea;">velocity-d</span>
  (<span style="color: #51afef;">lambda</span> (P)
    (ref P <span style="color: #a9a1e1;">0</span>)))
</pre>
</div>

<p>
which results in the parameter that is indexed at 0 in the inflated representation. And finally, this is the update function:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #51afef;">define</span> <span style="color: #dcaeea;">velocity-u</span>
  (<span style="color: #51afef;">lambda</span> (P g)
    (<span style="color: #51afef;">let</span> ((<span style="color: #dcaeea;">v</span> (<span style="color: #c678dd;">-</span> (<span style="color: #c678dd;">*</span> <span style="color: #c678dd;">mu</span> (ref P <span style="color: #a9a1e1;">1</span>)) (<span style="color: #c678dd;">*</span> alpha g))))
      (<span style="color: #c678dd;">list</span> (<span style="color: #c678dd;">+</span> (ref P <span style="color: #a9a1e1;">0</span>) v) v))))
</pre>
</div>

<p>
which expects an accompanied parameter \(P\) where the parameter \(P_0\) is accompanied by its velocity \(P_1\) from the last revision. The gradient gradient \(g\) is its second argument.
</p>
</div>
</div>
<div id="outline-container-org9596294" class="outline-6">
<h6 id="org9596294">RMSProp</h6>
<div class="outline-text-6" id="text-org9596294">
<p>
The <a href="#orgad48faf">momentum gradient descent</a> algorithm <b>improves the velocity of a revision by borrowing some velocity from the preceding revision</b>.<br />
There are other ways to improve the velocity of a revision. <b>RMSProp is an algorithms that works by modifying the fraction of the gradient used at each revision</b>.
</p>

<p>
We know that the gradient approaches 0.0 as we roll down to the bottom of the incline. Similarly to our problem of <a href="#orgd370e2c">velocity of descent</a>, <b>where the amount of change to each parameter gets smaller and smaller</b>, we observe the same behavior with the velocity of the gradient. In <a href="#orgad48faf">momentum gradient descent</a>, \(\alpha\) (the learning rate) is a constant, which causes the velocity of the gradient descent to slow down in a similar way.
</p>

<p>
Since \(\alpha\) represents the fraction of the gradient used as the velocity, another approach to addressing this problem is to make this fraction <i>adaptive</i>. Adaptive means that the fraction is decided based on the gradient and its historical values. To do that \(\alpha\) is multiplied with a factor \(D\) that reacts to the current gradient and it's historical values.<br />
The fraction of the gradient used as the velocity at every revision should reduce more slowly than the rate at which the gradient reduces, so the way in which \(D\) behaves means that it must get larger as the gradient gets smaller since \(\alpha\) itself is a constant.
</p>

<p>
We say that \(D\) varies <i>inversely</i> as the gradient. A simple way to make something vary inversely is to divide 1.0 by it. So \(D\) looks something like \[\frac{1}{G}\] Here \(G\) is referred to as a <i>modifier</i>.<br />
Now the task is to find \(G\), which must depend on the gradient and its history. Another thing to remember is that when multiplying \(\alpha\) by \(D\), we're doing \[\alpha * D = \alpha * \frac{1}{G} = \frac{\alpha}{G}\] So \(\alpha\) has to be divided by the modifier to change the fraction of the gradient we must use. This can be achieved by using something which takes the history of the gradient into account but is not susceptible to all the variations in it, so that its effect is not nullified.<br />
The solution is to use <a href="#org626d160">smoothing</a> to historically accumulate a modifier which is based on the gradient.
</p>

<p>
We need a representation with an accompaniment based on our smoothed gradients. Here's the definition of the <i>rms-u</i> function:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #51afef;">define</span> <span style="color: #dcaeea;">rms-u</span>
  (<span style="color: #51afef;">lambda</span> (P g)
    (<span style="color: #51afef;">let</span> ((<span style="color: #dcaeea;">r</span> (smooth beta (ref P <span style="color: #a9a1e1;">1</span>) (<span style="color: #c678dd;">sqr</span> g))))
      (<span style="color: #51afef;">let</span> ((<span style="color: #dcaeea;">alpha-hat</span> (<span style="color: #c678dd;">/</span> alpha (<span style="color: #c678dd;">+</span> (<span style="color: #c678dd;">sqrt</span> r) epsilon))))
        (<span style="color: #c678dd;">list</span> (<span style="color: #c678dd;">-</span> (ref P <span style="color: #a9a1e1;">0</span>) (<span style="color: #c678dd;">*</span> alpha-hat g)) r)))))
</pre>
</div>

<p>
Like other update functions, <i>rms-u</i> takes an accompanied parameter and a gradient and revises the accompanied parameter. Here's a breakdown of the function:
</p>

<ul class="org-ul">
<li>The accompaniment \(P_1\) is the smoothed value derived from the gradient \(g\).</li>
<li>\(r\) is the value we determine as the new accompaniment. It is part of the returned accompanied parameter.</li>
<li>\(\beta\) is a hyperparameter for the decay rate.</li>
<li><p>
The gradient \(g\) in the function call
</p>

<div class="org-src-container">
<pre class="src src-racket">(smooth beta (ref P <span style="color: #a9a1e1;">1</span>) (<span style="color: #c678dd;">sqr</span> g))
</pre>
</div>

<p>
has to be squared because \(g\) can be negative, and if we get too many consecutive negative gradients, then our historical averages can themselves become negative. This is a problem because r gets used by the modifier" G i.e.
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">+</span> (<span style="color: #c678dd;">sqrt</span> r) epsilon)
</pre>
</div>

<p>
and its being negative can make \(\hat{\alpha}\) (i.e. the learning rate) negative. When that happens, we end up ascending the gradient instead of descending it. This means that we would move our &theta; in a direction that <b>increases</b> the loss instead of a direction that <b>decreases</b> the loss.
</p></li>
<li><p>
In this expression:
</p>

<div class="org-src-container">
<pre class="src src-racket">((r (smooth beta (ref P <span style="color: #a9a1e1;">1</span>) (<span style="color: #c678dd;">sqr</span> g))))
</pre>
</div>

<p>
the problem with squares is that they grow much faster than the scalar that is being squared. Since \(r\) gets used inside
</p>

<div class="org-src-container">
<pre class="src src-racket">((alpha-hat (<span style="color: #c678dd;">/</span> alpha (<span style="color: #c678dd;">+</span> (<span style="color: #c678dd;">sqrt</span> r) epsilon))))
</pre>
</div>

<p>
as the modifier, the modified learning rate would increase at a faster than the rate at which the gradient reduce. This could cause the descent to <a href="#org0a803f9">overshoot</a> the lowest point in the loss curve. Therefore the idea of taking the square root with <i>sqrt</i>, is to modify \(r\) so that it tracks the gradient more closely instead of tracking the square of the gradient.<br />
However we need to account for the possibility that  \(r\) could be 0.0, which would cause the division of &alpha; to be undefined. This is easily solved, by adding a tiny constant &epsilon; known as the <i>stabilizer</i>, to (<i>sqrt</i> \(r\))
</p></li>
</ul>

<p>
Finally, those are the definitions for inflate and deflate functions:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #51afef;">define</span> <span style="color: #dcaeea;">rms-i</span>
  (<span style="color: #51afef;">lambda</span> (p)
    (<span style="color: #c678dd;">list</span> p (zeroes p))))
</pre>
</div>

<p>
for <i>inflate</i>, where we accompany \(p\) with \(r\). \(r\) is a zeroed tensor.<br />
And
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #51afef;">define</span> <span style="color: #dcaeea;">rms-d</span>
  (<span style="color: #51afef;">lambda</span> (P)
    (ref P <span style="color: #a9a1e1;">0</span>)))
</pre>
</div>

<p>
for <i>deflate</i>, where we extract the parameter \(p\) from the accompanied parameter.
</p>

<p>
This version of gradient descent is called RMSProp. The term <i>RMS</i> stands for <i>root mean square</i>, which reflects the fact that we use the <i>mean</i> (i.e. the smoothed historical average) of the <i>squares</i> and then take its square <i>root</i>. The suffix <i>Prop</i> is a contraction of the term <i>back propagation</i>.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org547febf" class="outline-4">
<h4 id="org547febf">Rate of change</h4>
<div class="outline-text-4" id="text-org547febf">
<ul class="org-ul">
<li><p>
<span class="underline">Rate of change short definition</span> <a id="org2f961e5"></a>
</p>

<p>
The rate of change of a function (of the <a href="#orgf252d17">objective</a> function in most cases), such as:
</p>

<div class="org-src-container">
<pre class="src src-racket">((l2-loss line) line-xs line-ys)
</pre>
</div>

<p>
determines how its result changes when its argument (i.e. &theta;) is revised.
</p>

<p>
The rate of change is also known as the <b>derivative</b>.
</p>

<p>
A more concrete example of this would be to invoke an objective function as such:
</p>

<div class="org-src-container">
<pre class="src src-racket">(((l2-loss line) line-xs line-ys) (<span style="color: #c678dd;">list</span> <span style="color: #a9a1e1;">0.0</span> <span style="color: #a9a1e1;">0.0</span>))
</pre>
</div>

<p>
which would achieve the objective of finding a well-fitted &theta; by returning the <a href="#orgd426ad4">loss</a> for this particular &theta;.
</p>

<p>
If the returned loss was, for instance, 33.21, we would test the behaviour of &theta;<sub>0</sub> to see how we should revise it. We then change &theta;<sub>0</sub> by increasing it a small amount for testing purposes, so that our new &theta;<sub>0</sub> is 0.0099. If the loss goes down, for instance, to 32.59, we are slightly closer to our ideal loss. In other words we changed the loss by:
</p>

<p>
(32.59 - 33.21) = -0.62
</p>

<p>
Now that increasing our &theta;<sub>0</sub> by 0.0099 has changed our loss by -0.62 we would say that our <b>rate of change</b> is:
</p>

<p>
-0.62 / 0.0099 = -62.63
</p></li>

<li><p>
<span class="underline">Rate of change calculation</span>
</p>

<p>
The rate of change is determined by subtracting the old (which so far has been greater) loss from the new (which so far has been smaller) loss, and in our examples so far this has resulted in negative values.
</p></li>

<li><p>
<span class="underline">Using the rate of change</span>
</p>

<p>
Increasing &theta; from 0.0 by a small value can result in a rate of change which has a large <b>absolute value</b>, meaning that a small increase in &theta; causes a relatively large decrease in its loss.
</p>

<p>
This idea can be used to determine how much further to revise &theta; so as to achieve a bigger loss. However we should be wary that the revision of &theta; moves us <b>closer</b> but does not <a id="org0a803f9"></a> <b>overshoot</b> the ideal loss.
</p>

<p>
This problem can be resolved by taking a small scalar (like 0.01), <b>and multiply the rate of change by it and revise our</b> &theta; <b>by that amount</b>.
</p>

<p>
This small scalar is known as the <b>learning rate</b>.
</p></li>

<li><p>
<span class="underline">&theta; <a id="org1e6608a"></a>revision after finding the rate of change</span>
</p>

<p>
The rate of change is multiplied by &alpha; (the <a id="org1848946"></a>learning rate) and the returned (negative) value used to update/revise &theta; by subtracting from &theta; this negative value (which has resulted in a positive revision of &theta; so far).
</p>

<p>
The rate of change <b>cannot</b> be reused as it depends on the current &theta;.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org626d160" class="outline-4">
<h4 id="org626d160">Smoothing</h4>
<div class="outline-text-4" id="text-org626d160">
<p>
To smooth a data set is to create an approximating function that attempts to capture important patterns in the data, while leaving out noise or other fine-scale structures/rapid phenomena. In smoothing, the data points of a signal are modified so individual points higher than the adjacent points (presumably because of noise) are reduced, and points that are lower than the adjacent points are increased leading to a smoother signal. (source: <a href="https://en.wikipedia.org/wiki/Smoothing">wikipedia</a>)
</p>

<p>
Here's a smoothing function:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #51afef;">define</span> <span style="color: #dcaeea;">smooth</span>
  (<span style="color: #51afef;">lambda</span> (decay-rate average g)
    (<span style="color: #c678dd;">+</span> (<span style="color: #c678dd;">*</span> decay-rate average)
       (<span style="color: #c678dd;">*</span> (<span style="color: #c678dd;">-</span> <span style="color: #a9a1e1;">1.0</span> decay-rate) g))))
</pre>
</div>

<p>
where <i>decay-rate</i> <b>must</b> always be a scalar between 0.0 and 1.0, <i>average</i> is a historically-accumulated average (or <i>historical average</i>), and \(g\) is a gradient. Both <i>average</i> and \(g\) must be tensors of the same shape. The function blends two tensors using <i>decay-rate</i> and
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">-</span> <span style="color: #a9a1e1;">1.0</span> decay-rate)
</pre>
</div>

<p>
<b>as weights</b>.<br />
Here's an example of how it is used using these seven scalars
</p>

<div class="org-src-container">
<pre class="src src-racket"><span style="color: #a9a1e1;">50.3</span> <span style="color: #a9a1e1;">22.7</span> <span style="color: #a9a1e1;">4.3</span> <span style="color: #a9a1e1;">2.7</span> <span style="color: #a9a1e1;">1.8</span> <span style="color: #a9a1e1;">2.2</span> <span style="color: #a9a1e1;">0.6</span>
</pre>
</div>

<p>
and finding (<i>smooth</i> 0.9 <span class="underline">0.0</span> 50.3)
</p>

<div class="org-src-container">
<pre class="src src-racket">(smooth <span style="color: #a9a1e1;">0.9</span> <span style="color: #a9a1e1;">0.0</span> <span style="color: #a9a1e1;">50.3</span>)

(<span style="color: #c678dd;">+</span> (<span style="color: #c678dd;">*</span> <span style="color: #a9a1e1;">0.9</span> <span style="color: #a9a1e1;">0.0</span>) (<span style="color: #c678dd;">*</span> <span style="color: #a9a1e1;">0.1</span> <span style="color: #a9a1e1;">50.3</span>))

(<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">0.0</span> <span style="color: #a9a1e1;">5.03</span>)

<span style="color: #51afef;">=&gt;</span> <span style="color: #a9a1e1;">5.03</span>
</pre>
</div>

<p>
Next, finding (<i>smooth</i> 0.9 <span class="underline">5.03</span> 22.7)
</p>

<div class="org-src-container">
<pre class="src src-racket">(smooth <span style="color: #a9a1e1;">0.9</span> <span style="color: #a9a1e1;">5.03</span> <span style="color: #a9a1e1;">22.7</span>)

(<span style="color: #c678dd;">+</span> (<span style="color: #c678dd;">*</span> <span style="color: #a9a1e1;">0.9</span> <span style="color: #a9a1e1;">5.03</span>) (<span style="color: #c678dd;">*</span> <span style="color: #a9a1e1;">0.1</span> <span style="color: #a9a1e1;">22.7</span>))

(<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">4.53</span> <span style="color: #a9a1e1;">2.27</span>)

<span style="color: #51afef;">=&gt;</span> <span style="color: #a9a1e1;">6.8</span>
</pre>
</div>

<p>
Once again, finding (<i>smooth</i> 0.9 <span class="underline">6.8</span> 4.3)
</p>

<div class="org-src-container">
<pre class="src src-racket">(smooth <span style="color: #a9a1e1;">0.9</span> <span style="color: #a9a1e1;">6.8</span> <span style="color: #a9a1e1;">4.3</span>)

(<span style="color: #c678dd;">+</span> (<span style="color: #c678dd;">*</span> <span style="color: #a9a1e1;">0.9</span> <span style="color: #a9a1e1;">6.8</span>) (<span style="color: #c678dd;">*</span> <span style="color: #a9a1e1;">0.1</span> <span style="color: #a9a1e1;">4.3</span>))

(<span style="color: #c678dd;">+</span> <span style="color: #a9a1e1;">6.12</span> <span style="color: #a9a1e1;">0.43</span>)

<span style="color: #51afef;">=&gt;</span> <span style="color: #a9a1e1;">6.55</span>
</pre>
</div>

<p>
Here are the original scalars
</p>

<div class="org-src-container">
<pre class="src src-racket"><span style="color: #a9a1e1;">50.3</span> <span style="color: #a9a1e1;">22.7</span> <span style="color: #a9a1e1;">4.3</span> <span style="color: #a9a1e1;">2.7</span> <span style="color: #a9a1e1;">1.8</span> <span style="color: #a9a1e1;">2.2</span> <span style="color: #a9a1e1;">0.6</span>
</pre>
</div>

<p>
and those are the historical averages after they've been smoothed
</p>

<div class="org-src-container">
<pre class="src src-racket"><span style="color: #a9a1e1;">5.03</span> <span style="color: #a9a1e1;">6.8</span> <span style="color: #a9a1e1;">6.55</span> <span style="color: #a9a1e1;">6.16</span> <span style="color: #a9a1e1;">6.07</span> <span style="color: #a9a1e1;">5.64</span> <span style="color: #a9a1e1;">5.14</span>
</pre>
</div>

<p>
Comparing the smoothed scalars to the original ones, one can tell that the smoothed ones don't vary as much and that the variations between them are much gentler.<br />
Repeatedly invoking <i>smooth</i> over these scalars <b>averages out variations by blending historical scalars with newer ones</b>. The importance of 0.9 that is passed as the <i>decay-rate</i> means that in order to find the new historical average, only 90% of the prior historical average is used, and every new scalar encountered is diminished to 10% of its value. Therefore, new scalars that vary wildly from the historical average contribute only to a small fraction to the new historical average.
</p>

<p>
Let's consider the first scalar 50.3. The first time <i>smooth</i> is invoked, it is multiplied by 0.1 and results in 5.03. The next time <i>smooth</i> is invoked, 5.03 is multiplied by 0.9. This means that the contribution of the <b>first</b> scalar to the next <b>smoothed</b> result is \[0.9*0.1*50.3=4.53\] meaning that it contributes less and less to the historical average as newer data is encountered.<br />
The <i>decay-rate</i> is referred to as the <i>rate-of-contribution</i> because the contribution of earlier items decays as more items are incorporated. The formula for the contribution of the scalar 50.3 after \(n\) invocations of <i>smooth</i> is \[0.9^{n-1}*0.1*50.3\]
</p>

<p>
Breaking down the formula:
</p>

<ul class="org-ul">
<li>\(0.9^{n-1}\): is the exponential decay factor where \(n\) is the time step. The base, 0.9, is the value decreases by 10% for each successive step. The exponent \(n-1\) is the factor which changes with each step of \(n\). The \(-1\) in this expression <b>arises from how indices are handled in time series or iterative processes</b>, where when \(n=1\) <b>is treated specially</b>. By using \(n-1\), the initial term (i.e. when \(n=1\)) which is \(0.9^0\), equals 1. This means that the influence of the initial value is fully retained at the start. <b>This is particularly important in smoothing applications where the first value has to maintain its full weight before starting the decay process</b>.</li>
<li><p>
\(0.1\): is the weighting factor, coming from
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">-</span> <span style="color: #a9a1e1;">1.0</span> decay-rate)
</pre>
</div></li>

<li>50.3: is the initial value.</li>
</ul>

<p>
In practical terms, it looks like this in the time series smoothing context:
</p>

<ul class="org-ul">
<li>At \(n=1\): \(0.9^{1-1}*0.1*50.3=1*0.1*50.3=5.03\)</li>
<li>At \(n=2\): \(0.9^{2-1}*0.1*50.3=0.9*0.1*50.3=4.527\)</li>
<li>At \(n=3\): \(0.9^{3-1}*0.1*50.3=0.81*0.1*50.3=4.0743\)</li>
</ul>

<p>
In general this formula is true for any scalar encountered. The contribution of that scalar decays over time. Smoothing is a method of incorporating historical information which is less relevant as we move forward, and it is true for tensors of any ranks.
</p>
</div>
</div>
<div id="outline-container-org6ba01ac" class="outline-4">
<h4 id="org6ba01ac">Tensors</h4>
<div class="outline-text-4" id="text-org6ba01ac">
<ul class="org-ul">
<li><p>
Here is a tensor<sup>1</sup>. A tensor<sup>1</sup> has only <b>scalars</b> and groups scalars together:
</p>

<div class="org-src-container">
<pre class="src src-racket">[<span style="color: #a9a1e1;">5.0</span> <span style="color: #a9a1e1;">7.18</span> <span style="color: #a9a1e1;">3.1416</span>]
</pre>
</div></li>

<li><p>
A tensor<sup>2</sup> can be thought of as a <b>matrix</b> or a <b>two-dimensional array</b>. The elements of a tensor<sup>2</sup> are tensors<sup>1</sup>, for example:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[<span style="color: #a9a1e1;">7</span> <span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">2</span> <span style="color: #a9a1e1;">5</span>] [<span style="color: #a9a1e1;">3</span> <span style="color: #a9a1e1;">8</span> <span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">9</span>] [<span style="color: #a9a1e1;">9</span> <span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">5</span>]]
</pre>
</div>

<p>
has 3 elements:
</p>

<div class="org-src-container">
<pre class="src src-racket">[<span style="color: #a9a1e1;">7</span> <span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">2</span> <span style="color: #a9a1e1;">5</span>]
[<span style="color: #a9a1e1;">3</span> <span style="color: #a9a1e1;">8</span> <span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">9</span>]
</pre>
</div>

<p>
and:
</p>

<div class="org-src-container">
<pre class="src src-racket">[<span style="color: #a9a1e1;">9</span> <span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">6</span> <span style="color: #a9a1e1;">5</span>]
</pre>
</div>

<p>
Therefore if we have a tensor whose elements are tensors<sup>m</sup>, that makes it a tensor<sup>m+1</sup>. One condition however is that all the tensors<sup>m</sup> must have the same numbers of elements.
</p></li>

<li><p>
A scalar such as:
</p>

<div class="org-src-container">
<pre class="src src-racket"><span style="color: #a9a1e1;">9</span>
</pre>
</div>

<p>
is also a tensor. It is a tensor<sup>0</sup>, but <b>zero-dimensional</b> arrays are rarely mentioned.
</p></li>
</ul>
</div>
<div id="outline-container-org83b62ec" class="outline-5">
<h5 id="org83b62ec">Tensor rank</h5>
<div class="outline-text-5" id="text-org83b62ec">
<ul class="org-ul">
<li><p>
The above superscripts have a name, they are known as the <b>rank</b> of the tensor. The rank of a tensor tells us how deeply nested its elements are. For instance, here is a tensor<sup>3</sup>:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[[<span style="color: #a9a1e1;">8</span> <span style="color: #a9a1e1;">9</span>] [<span style="color: #a9a1e1;">4</span> <span style="color: #a9a1e1;">7</span>]]]
</pre>
</div>

<p>
this is because it has 1 tensor<sup>2</sup> element that has 2 tensor<sup>1</sup> elements of 2 scalars each.
</p></li>

<li><p>
In any given tensor, the nested tensors have the same number of elements. For example, the nested tensors of tensors<sup>2</sup> are all tensors<sup>1</sup>, and each of those tensors<sup>1</sup> has the same number of tensors<sup>0</sup>.
</p>

<p>
This means that the tensors<sup>m</sup> that are elements of a tensor<sup>m+1</sup> have the same <b>shape</b>.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-orgc5f34e0" class="outline-5">
<h5 id="orgc5f34e0">Tensor shape</h5>
<div class="outline-text-5" id="text-orgc5f34e0">
<ul class="org-ul">
<li><p>
The <b>shape</b> of:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[<span style="color: #a9a1e1;">5.2</span> <span style="color: #a9a1e1;">6.3</span> <span style="color: #a9a1e1;">8.0</span>] [<span style="color: #a9a1e1;">6.9</span> <span style="color: #a9a1e1;">7.1</span> <span style="color: #a9a1e1;">0.5</span>]]
</pre>
</div>

<p>
is this list of positive natural numbers:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">list</span> <span style="color: #a9a1e1;">2</span> <span style="color: #a9a1e1;">3</span>)
</pre>
</div>

<p>
because it is a tensor<sup>2</sup> of 2 tensors<sup>1</sup>, each of which has 3 tensors<sup>0</sup> elements.
</p>

<p>
The shape of:
</p>

<div class="org-src-container">
<pre class="src src-racket">[[[<span style="color: #a9a1e1;">5</span>] [<span style="color: #a9a1e1;">6</span>] [<span style="color: #a9a1e1;">8</span>]] [[<span style="color: #a9a1e1;">7</span>] [<span style="color: #a9a1e1;">9</span>] [<span style="color: #a9a1e1;">5</span>]]]
</pre>
</div>

<p>
is:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #c678dd;">list</span> <span style="color: #a9a1e1;">2</span> <span style="color: #a9a1e1;">3</span> <span style="color: #a9a1e1;">1</span>)
</pre>
</div>

<p>
It is:
</p>

<ul class="org-ul">
<li>a tensor<sup>3</sup> of 2 tensor<sup>2</sup> elements.</li>

<li>Each of those tensor<sup>2</sup> has 3 tensor<sup>1</sup> elements.</li>

<li>Each of those tensor<sup>1</sup> has 1 tensor<sup>0</sup> element, which is a scalar.</li>
</ul></li>

<li>Another useful thing to note is that the rank of a tensor is equal to the length of its shape.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org50893a4" class="outline-4">
<h4 id="org50893a4">Update of parameters</h4>
<div class="outline-text-4" id="text-org50893a4">
<p>
The following function:
</p>

<div class="org-src-container">
<pre class="src src-racket">(<span style="color: #51afef;">define</span> <span style="color: #dcaeea;">naked-u</span>
  (<span style="color: #51afef;">lambda</span> (P g)
    (&#8722; P (<span style="color: #c678dd;">*</span> alpha g))))
</pre>
</div>

<p>
updates the parameters by multiplying the gradient g by the learning rate alpha, and subtracts the result from the parameter P to yield the next P, so that ultimately we get closer to a well-fitted &theta;.
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: dm</p>
<p class="date">Created: 2024-06-22 Sat 08:42</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
